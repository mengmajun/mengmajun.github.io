> [cleanrl dqn_atari.py ä»£ç ](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py#L108)
> [cleanrl aratiæ¸¸æˆ dqnè¯•éªŒç»“æœ](https://docs.cleanrl.dev/rl-algorithms/dqn/)


## ğŸ§± ç½‘ç»œç»“æ„è¯¦è§£ï¼šä»å›¾åƒè¾“å…¥åˆ° Q å€¼è¾“å‡º

```python
class QNetwork(nn.Module):
    def __init__(self, env):
        super().__init__()
        self.network = nn.Sequential(
            nn.Conv2d(4, 32, 8, stride=4),      # Layer 1: Conv
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2),     # Layer 2: Conv
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, stride=1),     # Layer 3: Conv
            nn.ReLU(),
            nn.Flatten(),                       # Layer 4: Flatten
            nn.Linear(3136, 512),               # Layer 5: FC
            nn.ReLU(),
            nn.Linear(512, env.single_action_space.n),  # Layer 6: Output
        )
```

ä¸‹é¢æˆ‘ä»¬ä¸€å±‚ä¸€å±‚åˆ†æï¼š

### âœ… è¾“å…¥æ ¼å¼ï¼šå †å çš„ç°åº¦å¸§

é€šè¿‡ç¯å¢ƒåŒ…è£…å™¨å¤„ç†åï¼Œè¾“å…¥çŠ¶æ€æ˜¯ï¼š
- **4 å¸§ç°åº¦å›¾åƒ**
- æ¯å¸§å¤§å° `(84, 84)`
- æ‰€ä»¥æ•´ä½“è¾“å…¥ shape ä¸ºï¼š`(batch_size, 4, 84, 84)`  
  ï¼ˆPyTorch è¦æ±‚ channel åœ¨å‰ï¼‰

> ğŸ‘‰ è¿™é‡Œçš„ `4` ä¸æ˜¯ RGB çš„ä¸‰é€šé“ï¼Œè€Œæ˜¯**æ—¶é—´ä¸Šçš„ 4 ä¸ªè¿ç»­å¸§**ï¼Œç”¨äºæ„ŸçŸ¥è¿åŠ¨æ–¹å‘ã€‚

---

### ğŸ” ç¬¬1å±‚ï¼š`nn.Conv2d(4, 32, 8, stride=4)`

- è¾“å…¥é€šé“ï¼š4ï¼ˆ4 å¸§ï¼‰
- è¾“å‡ºé€šé“ï¼š32ï¼ˆæå– 32 ç§ç‰¹å¾å›¾ï¼‰
- å·ç§¯æ ¸å¤§å°ï¼š8Ã—8
- æ­¥é•¿ï¼š4

è®¡ç®—è¾“å‡ºå°ºå¯¸ï¼š
$$
\frac{84 - 8}{4} + 1 = 20
$$
âœ… è¾“å‡º shapeï¼š`(batch_size, 32, 20, 20)`

---

### ğŸ” ç¬¬2å±‚ï¼š`nn.Conv2d(32, 64, 4, stride=2)`

- è¾“å…¥ï¼š`(32, 20, 20)`
- å·ç§¯æ ¸ï¼š4Ã—4ï¼Œæ­¥é•¿ 2

è¾“å‡ºå°ºå¯¸ï¼š
$$
\frac{20 - 4}{2} + 1 = 9
$$
âœ… è¾“å‡º shapeï¼š`(batch_size, 64, 9, 9)`

---

### ğŸ” ç¬¬3å±‚ï¼š`nn.Conv2d(64, 64, 3, stride=1)`

- è¾“å…¥ï¼š`(64, 9, 9)`
- å·ç§¯æ ¸ï¼š3Ã—3ï¼Œæ­¥é•¿ 1

è¾“å‡ºå°ºå¯¸ï¼š
$$
\frac{9 - 3}{1} + 1 = 7
$$
âœ… è¾“å‡º shapeï¼š`(batch_size, 64, 7, 7)`

---

### ğŸ“¦ ç¬¬4å±‚ï¼š`nn.Flatten()`

å°†æ‰€æœ‰ç»´åº¦å±•å¹³æˆä¸€ç»´å‘é‡ã€‚

å½“å‰ä½“ç§¯ï¼š`64 Ã— 7 Ã— 7 = 3136`

âœ… å±•å¹³å shapeï¼š`(batch_size, 3136)`

---

### ğŸ’¡ ç¬¬5å±‚ï¼š`nn.Linear(3136, 512)`

å…¨è¿æ¥å±‚ï¼ŒæŠŠ 3136 ç»´å‹ç¼©åˆ° 512 ç»´ï¼Œè¿›è¡Œé«˜çº§ç‰¹å¾æ•´åˆã€‚

âœ… è¾“å‡º shapeï¼š`(batch_size, 512)`

---

### ğŸ¯ ç¬¬6å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰ï¼š`nn.Linear(512, env.single_action_space.n)`

è¿™æ‰æ˜¯æœ€å…³é”®çš„ï¼

- è¾“å…¥ï¼š512 ç»´ç‰¹å¾å‘é‡
- è¾“å‡ºï¼šç­‰äºåŠ¨ä½œç©ºé—´çš„å¤§å°

#### ç¤ºä¾‹ï¼šä¸åŒæ¸¸æˆçš„åŠ¨ä½œæ•°
| æ¸¸æˆ | åŠ¨ä½œæ•°é‡ |
|------|---------|
| `BreakoutNoFrameskip-v4` | 4ï¼ˆNOOP, FIRE, LEFT, RIGHTï¼‰ |
| `PongNoFrameskip-v4` | 6ï¼ˆä½†å®é™…å¸¸ç”¨ 3 æˆ– 4ï¼‰ |
| `CartPole-v1` | 2ï¼ˆå·¦æ¨ã€å³æ¨ï¼‰ |

**`QNetwork` çš„æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¡¨ç¤ºåœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œæ¯ä¸ªå¯èƒ½åŠ¨ä½œçš„ Q å€¼ä¼°è®¡ã€‚**

ä¾‹å¦‚ï¼Œåœ¨ `BreakoutNoFrameskip-v4` æ¸¸æˆä¸­ï¼š
- åŠ¨ä½œç©ºé—´æœ‰ 4 ç§ï¼šä¸åŠ¨ã€å·¦ç§»ã€å³ç§»ã€å¼€çƒï¼ˆfireï¼‰
- é‚£ä¹ˆç½‘ç»œè¾“å‡ºå°±æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 4 çš„å‘é‡ï¼Œå½¢å¦‚ï¼š

```python
[ 2.1, -0.5, 3.8, 1.0 ]
```

è¿™è¡¨ç¤ºï¼š
- åŠ¨ä½œ 0ï¼ˆä¸åŠ¨ï¼‰ï¼šä»·å€¼ 2.1
- åŠ¨ä½œ 1ï¼ˆå·¦ç§»ï¼‰ï¼šä»·å€¼ -0.5
- åŠ¨ä½œ 2ï¼ˆå³ç§»ï¼‰ï¼šä»·å€¼ 3.8 â† æœ€é«˜ â†’ æ™ºèƒ½ä½“ä¼šå€¾å‘äºé€‰æ‹©è¿™ä¸ªåŠ¨ä½œ
- åŠ¨ä½œ 3ï¼ˆå¼€çƒï¼‰ï¼šä»·å€¼ 1.0

ğŸ¯ æ‰€ä»¥ï¼Œ**è¾“å‡ºçš„ shape æ˜¯ `[N, num_actions]`**ï¼Œå…¶ä¸­ N æ˜¯ batch sizeã€‚


---

### ğŸ§ª å‰å‘ä¼ æ’­ï¼š`forward` å‡½æ•°åšäº†ä»€ä¹ˆï¼Ÿ

```python
def forward(self, x):
    return self.network(x / 255.0)
```

è¿™é‡Œåªåšäº†ä¸€ä»¶äº‹ï¼š**å°†åƒç´ å€¼å½’ä¸€åŒ–åˆ° [0, 1] åŒºé—´**

åŸå§‹å›¾åƒåƒç´ èŒƒå›´æ˜¯ 0~255ï¼Œé™¤ä»¥ 255 åå˜ä¸º 0~1ï¼Œæœ‰åˆ©äºç¥ç»ç½‘ç»œè®­ç»ƒç¨³å®šã€‚

ğŸ“Œ è¾“å…¥ `x` çš„ shapeï¼š`(batch_size, 4, 84, 84)`  
ğŸ“Œ è¾“å‡º `q_values` çš„ shapeï¼š`(batch_size, num_actions)`

---

## âœ… ä¸¾ä¸ªå…·ä½“ä¾‹å­ï¼šBreakout æ¸¸æˆä¸­çš„è¾“å‡º

å‡è®¾ä½ è¿è¡Œçš„æ˜¯ï¼š

```bash
python dqn_atari.py --env-id BreakoutNoFrameskip-v4
```

é‚£ä¹ˆï¼š
- `env.single_action_space.n == 4`
- ç½‘ç»œè¾“å‡ºå°±æ˜¯ shape ä¸º `(1, 4)` æˆ– `(32, 4)` çš„å¼ é‡ï¼ˆå–å†³äºæ˜¯å¦æ‰¹å¤„ç†ï¼‰

æ¯”å¦‚æŸæ¬¡å‰å‘ä¼ æ’­ç»“æœï¼š

```python
q_values = tensor([[ 1.2, -0.3,  3.5,  0.8 ]])  # shape: [1, 4]
```

ç„¶åä»£ç ä¸­è¿™æ ·é€‰åŠ¨ä½œï¼š

```python
actions = torch.argmax(q_values, dim=1).cpu().numpy()
# â†’ argmax([1.2, -0.3, 3.5, 0.8]) = 2 â†’ è¡¨ç¤ºâ€œå‘å³ç§»åŠ¨â€
```

---

## ğŸ“Œ æ€»ç»“ï¼š`QNetwork` è¾“å‡ºè¯¦è§£

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **è¾“å‡ºç±»å‹** | åŠ¨ä½œä»·å€¼å‡½æ•° $Q(s,a)$ çš„ä¼°è®¡ |
| **è¾“å‡ºå½¢å¼** | å¼ é‡ï¼ˆTensorï¼‰ï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªçŠ¶æ€çš„æ‰€æœ‰åŠ¨ä½œ Q å€¼ |
| **è¾“å‡º shape** | `(batch_size, num_actions)` |
| **æ•°å€¼å«ä¹‰** | æ•°å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºæ‰§è¡Œè¯¥åŠ¨ä½œçš„é¢„æœŸå›æŠ¥è¶Šé«˜ |
| **æ˜¯å¦å¸¦ softmaxï¼Ÿ** | âŒ ä¸æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼åªæ˜¯åŸå§‹å¾—åˆ†ï¼ˆlogitsï¼‰ |
| **æ˜¯å¦éœ€è¦å½’ä¸€åŒ–ï¼Ÿ** | âŒ ä¸éœ€è¦ï¼Œç›´æ¥ç”¨äº argmax æˆ– loss è®¡ç®— |

---



## ğŸ§­ æ€»è§ˆï¼šDQN æ ¸å¿ƒæµç¨‹ï¼ˆäº”æ­¥å¾ªç¯ï¼‰

1. **åˆå§‹åŒ–**ï¼šæ„å»º Q ç½‘ç»œã€ç›®æ ‡ç½‘ç»œã€ç»éªŒå›æ”¾ç¼“å†²åŒº
2. **ä¸ç¯å¢ƒäº¤äº’**ï¼šç”¨ Îµ-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œï¼Œæ”¶é›†ç»éªŒ
3. **å­˜å‚¨ç»éªŒ**ï¼šå°† `(s, a, r, s', done)` å­˜å…¥ Replay Buffer
4. **é‡‡æ ·è®­ç»ƒ**ï¼šä» buffer ä¸­éšæœºæŠ½æ ·ä¸€æ‰¹æ•°æ®è¿›è¡Œå­¦ä¹ 
5. **æ›´æ–°ç½‘ç»œ**ï¼š
   - ç”¨ MSE æŸå¤±æ›´æ–°ä¸»ç½‘ç»œ
   - å®šæœŸåŒæ­¥æˆ–è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ

ä¸‹é¢æˆ‘ä»¬ä¸€æ­¥æ­¥æ‹†è§£ã€‚

---

## 1ï¸âƒ£ é˜¶æ®µä¸€ï¼šåˆå§‹åŒ– Setup

```python
# env setup
envs = gym.vector.SyncVectorEnv(
    [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]
)
assert isinstance(envs.single_action_space, gym.spaces.Discrete), "only discrete action space is supported"

q_network = QNetwork(envs).to(device)
optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)
target_network = QNetwork(envs).to(device)
target_network.load_state_dict(q_network.state_dict())

rb = ReplayBuffer(
    args.buffer_size,
    envs.single_observation_space,
    envs.single_action_space,
    device,
    optimize_memory_usage=True,
    handle_timeout_termination=False,
)
```

### âœ… å¯¹åº”åŠŸèƒ½è¯´æ˜ï¼š

| ç»„ä»¶ | åŠŸèƒ½ |
|------|------|
| `SyncVectorEnv` + `make_env` | åˆ›å»ºæ¸¸æˆç¯å¢ƒï¼Œå¹¶åº”ç”¨ Atari é¢„å¤„ç†ï¼ˆç°åº¦ã€ç¼©æ”¾ã€è·³å¸§ã€å †å ï¼‰ |
| `QNetwork` | ä¸»ç½‘ç»œï¼Œç”¨äºé¢„æµ‹å½“å‰çŠ¶æ€ä¸‹çš„ Q å€¼ |
| `target_network` | ç›®æ ‡ç½‘ç»œï¼Œç”¨äºç¨³å®š TD ç›®æ ‡è®¡ç®— |
| `optimizer` | Adam ä¼˜åŒ–å™¨ï¼Œè´Ÿè´£æ¢¯åº¦æ›´æ–° |
| `ReplayBuffer` | ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œä¿å­˜å†å²ç»éªŒä¾›åç»­å¤ç”¨ |

ğŸ“Œ æ³¨æ„ï¼šç›®æ ‡ç½‘ç»œåˆå§‹åŒ–æ—¶å’Œä¸»ç½‘ç»œå‚æ•°å®Œå…¨ç›¸åŒ â†’ ä¿è¯åˆå§‹ç›®æ ‡å¯ä¿¡

---

## 2ï¸âƒ£ é˜¶æ®µäºŒï¼šä¸ç¯å¢ƒäº¤äº’ï¼ˆAction Selectionï¼‰

```python
obs, _ = envs.reset(seed=args.seed)
for global_step in range(args.total_timesteps):
    # ALGO LOGIC: put action logic here
    epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)
    if random.random() < epsilon:
        actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])
    else:
        q_values = q_network(torch.Tensor(obs).to(device))
        actions = torch.argmax(q_values, dim=1).cpu().numpy()
```

### âœ… å¯¹åº” DQN æ­¥éª¤ï¼š**Îµ-greedy åŠ¨ä½œé€‰æ‹©**

- ä½¿ç”¨ `linear_schedule` å®ç° Îµ çš„çº¿æ€§è¡°å‡ï¼š
  - åˆå§‹æ¢ç´¢ç‡ `start_e=1.0` â†’ å®Œå…¨éšæœº
  - æœ€ç»ˆæ¢ç´¢ç‡ `end_e=0.01` â†’ å‡ ä¹è´ªå©ª
  - åœ¨å‰ 10% çš„è®­ç»ƒæ­¥æ•°å†…å®Œæˆè¡°å‡

ğŸ§  ç±»æ¯”ï¼šå°å­©åˆšå¼€å§‹ä¹±è¯•ï¼Œè¶Šé•¿å¤§è¶Šä¾èµ–ç»éªŒ

- åŠ¨ä½œé€‰æ‹©é€»è¾‘ï¼š
  - è‹¥éšæœº < Îµ â†’ éšæœºåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
  - å¦åˆ™ â†’ è¾“å…¥å½“å‰è§‚æµ‹ `obs` åˆ° Q ç½‘ç»œ â†’ å–æœ€å¤§ Q å€¼çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰

> ğŸ’¡ è¾“å…¥ `obs` æ˜¯ shape ä¸º `(1, 4, 84, 84)` çš„ 4 å¸§å †å å›¾åƒ

---

## 3ï¸âƒ£ é˜¶æ®µä¸‰ï¼šæ‰§è¡ŒåŠ¨ä½œ & å­˜å‚¨ç»éªŒ

```python
next_obs, rewards, terminations, truncations, infos = envs.step(actions)

# ... å¤„ç† truncation çš„ final_observation ...
real_next_obs = next_obs.copy()
for idx, trunc in enumerate(truncations):
    if trunc:
        real_next_obs[idx] = infos["final_observation"][idx]

rb.add(obs, real_next_obs, actions, rewards, terminations, infos)
obs = next_obs  # æ›´æ–°çŠ¶æ€
```

### âœ… å¯¹åº” DQN æ­¥éª¤ï¼š**æ”¶é›†ç»éªŒå¹¶å­˜å…¥ Replay Buffer**

- `envs.step(actions)`ï¼šæ‰§è¡ŒåŠ¨ä½œï¼Œè·å¾—åé¦ˆ
- `rb.add(...)`ï¼šå°†äº”å…ƒç»„ `(s, s', a, r, done)` å­˜å…¥ç»éªŒæ± 
- ç‰¹åˆ«å¤„ç†äº† `truncation`ï¼ˆæˆªæ–­ï¼‰æƒ…å†µï¼Œç¡®ä¿ `real_next_obs` æ­£ç¡®è®¾ç½®
- æœ€åæ›´æ–° `obs = next_obs`ï¼Œè¿›å…¥ä¸‹ä¸€æ—¶é—´æ­¥

ğŸ“Œ å…³é”®ç‚¹ï¼š
> æ‰€æœ‰ç»éªŒéƒ½å…ˆå­˜èµ·æ¥ï¼Œä¸ç«‹å³è®­ç»ƒ â†’ æ”¯æŒåç»­**ç¦»çº¿æ‰¹é‡å­¦ä¹ **

---

## 4ï¸âƒ£ é˜¶æ®µå››ï¼šè®­ç»ƒé˜¶æ®µï¼ˆLearning from Experienceï¼‰

```python
if global_step > args.learning_starts:
    if global_step % args.train_frequency == 0:
        data = rb.sample(args.batch_size)
```

### âœ… æ¡ä»¶åˆ¤æ–­å«ä¹‰ï¼š

- `global_step > args.learning_starts`ï¼šé¢„çƒ­æœŸè¿‡åæ‰å¼€å§‹è®­ç»ƒï¼ˆé»˜è®¤ 80,000 æ­¥ï¼‰
  - ç›®çš„ï¼šè®© buffer å…ˆç§¯ç´¯è¶³å¤Ÿå¤šçš„ç»éªŒ
- `global_step % args.train_frequency == 0`ï¼šæ¯ 4 æ­¥è®­ç»ƒä¸€æ¬¡ï¼ˆå¯è°ƒï¼‰
  - èŠ‚çœè®¡ç®—èµ„æºï¼Œé¿å…é¢‘ç¹æ›´æ–°

- `data = rb.sample(args.batch_size)`ï¼šä» replay buffer ä¸­éšæœºæŠ½å–ä¸€ä¸ª batchï¼ˆé»˜è®¤ 32 æ¡ç»éªŒï¼‰

ğŸ“Œ æ•°æ®ç»“æ„ç¤ºä¾‹ï¼š
```python
data.observations     # shape: [32, 4, 84, 84]
data.actions          # shape: [32, 1]
data.rewards          # shape: [32, 1]
data.next_observations# shape: [32, 4, 84, 84]
data.dones            # shape: [32, 1]
```

---

### âœ… è®¡ç®— TD Targetï¼ˆè´å°”æ›¼ç›®æ ‡ï¼‰

```python
with torch.no_grad():
    target_max, _ = target_network(data.next_observations).max(dim=1)
    td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())
```

ğŸ¯ è¿™æ˜¯æ•´ä¸ª DQN çš„æ ¸å¿ƒå…¬å¼ï¼

$$
y_t = r_t + \gamma \cdot \max_{a'} Q_{\text{target}}(s_{t+1}, a') \cdot (1 - \text{done}_t)
$$

é€é¡¹è§£é‡Šï¼š

| é¡¹ | å«ä¹‰ |
|-----|--------|
| `target_network(...)` | ç”¨**ç›®æ ‡ç½‘ç»œ**é¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„æ‰€æœ‰ Q å€¼ |
| `.max(dim=1)` | å–æœ€å¤§å€¼ â†’ å¾—åˆ°æœ€ä¼˜åŠ¨ä½œå¯¹åº”çš„ Q å€¼ |
| `args.gamma * target_max` | åŠ ä¸ŠæŠ˜æ‰£åçš„æœªæ¥ä»·å€¼ |
| `* (1 - data.dones)` | å¦‚æœ episode å·²ç»“æŸï¼ˆdone=Trueï¼‰ï¼Œåˆ™æœªæ¥ä»·å€¼ä¸º 0 |
| `with torch.no_grad()` | ä¸è®°å½•æ¢¯åº¦ â†’ æé«˜æ•ˆç‡ä¸”é˜²æ­¢åå‘ä¼ æ’­æ±¡æŸ“ç›®æ ‡ç½‘ç»œ |

ğŸ“Œ è¿™é‡Œä½¿ç”¨çš„æ˜¯ **Hard Update + å›ºå®šç›®æ ‡ç½‘ç»œ**ï¼Œä¸æ˜¯ Double DQNï¼ˆä½†ç»“æ„å·²æ”¯æŒæ‰©å±•ï¼‰

---

### âœ… è®¡ç®—å½“å‰ Q å€¼ä¼°è®¡

```python
old_val = q_network(data.observations).gather(1, data.actions).squeeze()
```

è¿™ä¸€æ­¥æ˜¯åœ¨è®¡ç®—ï¼š
$$
Q(s_t, a_t; \theta)
$$

å…·ä½“æ“ä½œï¼š
- `q_network(...)`ï¼šä¸»ç½‘ç»œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„ Q å€¼ï¼Œshape `[32, num_actions]`
- `.gather(1, data.actions)`ï¼šé€‰å‡ºå®é™…é‡‡å–çš„é‚£ä¸ªåŠ¨ä½œçš„ Q å€¼ï¼Œshape `[32, 1]`
- `.squeeze()`ï¼šå‹æˆ `[32]`ï¼Œä¾¿äºåç»­ loss è®¡ç®—

ğŸ“Œ æ³¨æ„ï¼šè¿™é‡Œåªæ›´æ–°è¢«é€‰ä¸­çš„åŠ¨ä½œçš„ Q å€¼ â†’ ç¬¦åˆ Q-learning çš„æ›´æ–°åŸåˆ™

---

### âœ… è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­

```python
loss = F.mse_loss(td_target, old_val)

optimizer.zero_grad()
loss.backward()
optimizer.step()
```

æŸå¤±å‡½æ•°å°±æ˜¯å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼š

$$
\mathcal{L}(\theta) = \mathbb{E}\left[ (y_t - Q(s_t, a_t; \theta))^2 \right]
$$

ç„¶åæ ‡å‡†çš„ PyTorch è®­ç»ƒä¸‰è¿ï¼š
1. æ¸…é™¤æ¢¯åº¦
2. åå‘ä¼ æ’­
3. æ›´æ–°å‚æ•°

ğŸ“Œ æ—¥å¿—è®°å½•ï¼š
```python
writer.add_scalar("losses/td_loss", loss, global_step)
writer.add_scalar("losses/q_values", old_val.mean().item(), global_step)
```
- `td_loss`ï¼šè¡¡é‡é¢„æµ‹è¯¯å·®å¤§å°
- `q_values`ï¼šç›‘æ§æ˜¯å¦å‡ºç°è¿‡ä¼°è®¡æˆ–æ¬ ä¼°è®¡

---

## 5ï¸âƒ£ é˜¶æ®µäº”ï¼šæ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆTarget Network Updateï¼‰

```python
if global_step % args.target_network_frequency == 0:
    for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):
        target_network_param.data.copy_(
            args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data
        )
```

### âœ… è¿™æ˜¯ Polyak Soft Updateï¼ˆè½¯æ›´æ–°ï¼‰

å…¬å¼ä¸ºï¼š
$$
\theta_{\text{target}} \leftarrow \tau \cdot \theta + (1 - \tau) \cdot \theta_{\text{target}}
$$

- å½“ `tau=1.0` æ—¶ â†’ ç­‰ä»·äº**ç¡¬æ›´æ–°**ï¼ˆæ¯éš” 1000 æ­¥å®Œå…¨å¤åˆ¶ä¸€æ¬¡ï¼‰
- å½“ `tau<1.0` æ—¶ â†’ å®ç°å¹³æ»‘è¿‡æ¸¡ï¼Œè¿›ä¸€æ­¥æå‡ç¨³å®šæ€§

ğŸ“Œ é»˜è®¤ `tau=1.0`ï¼Œæ‰€ä»¥æ˜¯æ¯ 1000 æ­¥åšä¸€æ¬¡ç¡¬æ›´æ–°

---

## ğŸ“Š æ—¥å¿—ä¸è¯„ä¼°éƒ¨åˆ†ï¼ˆè¾…åŠ©åŠŸèƒ½ï¼‰

```python
if "final_info" in infos:
    for info in infos["final_info"]:
        if info and "episode" in info:
            print(f"global_step={global_step}, episodic_return={info['episode']['r']}")
            writer.add_scalar("charts/episodic_return", info["episode"]["r"], global_step)
```

- æ¯å½“ä¸€ä¸ª episode ç»“æŸï¼Œè®°å½•å›åˆå›æŠ¥ï¼ˆreward sumï¼‰
- ç”¨äºç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼Œè§‚å¯Ÿæ™ºèƒ½ä½“æ˜¯å¦åœ¨è¿›æ­¥

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜ä¸è¯„ä¼°

```python
if args.save_model:
    torch.save(q_network.state_dict(), model_path)
    episodic_returns = evaluate(...)  # ä½¿ç”¨æµ‹è¯•æ¨¡å¼è¿è¡Œ 10 åœºæ¸¸æˆ
    for idx, ret in enumerate(episodic_returns):
        writer.add_scalar("eval/episodic_return", ret, idx)
```

- è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹æƒé‡
- è°ƒç”¨ `evaluate` å‡½æ•°ï¼Œåœ¨ç‹¬ç«‹ç¯å¢ƒä¸­æµ‹è¯•æ€§èƒ½ï¼ˆÎµ=0.01ï¼Œå‡å°‘éšæœºæ€§ï¼‰
- å°†ç»“æœä¸Šä¼ è‡³ TensorBoard å’Œ Hugging Face Hubï¼ˆå¯é€‰ï¼‰

---

## âœ… æ€»ç»“ï¼šDQN æµç¨‹ä¸ä»£ç æ˜ å°„è¡¨

| DQN æ­¥éª¤ | å¯¹åº”ä»£ç ä½ç½® | å…³é”®å®ç° |
|---------|---------------|-----------|
| åˆå§‹åŒ–ç½‘ç»œ | `q_network`, `target_network` | CNN æ¶æ„ï¼ŒAdam ä¼˜åŒ–å™¨ |
| åˆå§‹åŒ–ç»éªŒæ±  | `ReplayBuffer` | æ”¯æŒé«˜æ•ˆé‡‡æ ·å’Œ truncation å¤„ç† |
| Îµ-greedy åŠ¨ä½œé€‰æ‹© | `epsilon = linear_schedule(...)` | çº¿æ€§è¡°å‡ï¼Œé¼“åŠ±å‰æœŸæ¢ç´¢ |
| æ‰§è¡ŒåŠ¨ä½œ | `envs.step(actions)` | è·å–æ–°çŠ¶æ€å’Œå¥–åŠ± |
| å­˜å‚¨ç»éªŒ | `rb.add(...)` | å†™å…¥ replay buffer |
| é‡‡æ · batch | `rb.sample(batch_size)` | éšæœºæŠ½æ ·ï¼Œæ‰“ç ´ç›¸å…³æ€§ |
| è®¡ç®— TD ç›®æ ‡ | `target_network(...).max()` | ä½¿ç”¨ç›®æ ‡ç½‘ç»œé˜²æ­¢è‡ªä¸¾æ¼‚ç§» |
| è®¡ç®—å½“å‰ Q å€¼ | `q_network(...).gather(...)` | æå–æ‰€é€‰åŠ¨ä½œçš„ä¼°è®¡å€¼ |
| è®¡ç®—æŸå¤± | `F.mse_loss(td_target, old_val)` | å›å½’æŸå¤±é©±åŠ¨å­¦ä¹  |
| åå‘ä¼ æ’­ | `loss.backward()`, `optimizer.step()` | æ›´æ–°ä¸»ç½‘ç»œå‚æ•° |
| æ›´æ–°ç›®æ ‡ç½‘ç»œ | `copy_ weights every C steps` | ç¡¬æ›´æ–° / è½¯æ›´æ–°ï¼ˆPolyakï¼‰ |
| æ—¥å¿—è®°å½• | `SummaryWriter` | è®°å½• lossã€SPSã€return ç­‰æŒ‡æ ‡ |
| æ¨¡å‹è¯„ä¼° | `evaluate(...)` | ç‹¬ç«‹æµ‹è¯•ï¼ŒéªŒè¯æ³›åŒ–èƒ½åŠ› |

---

## ğŸ¯ ä¸€å¥è¯æ€»ç»“

> è¿™æ®µ `dqn_atari.py` ä»£ç å®Œæ•´å®ç°äº† DQN çš„æ‰€æœ‰æ ¸å¿ƒæŠ€æœ¯ï¼š
>
> **ç”¨å·ç§¯ç½‘ç»œç†è§£åƒç´  â†’ ç”¨ç»éªŒå›æ”¾æ‰“ç ´åºåˆ—ç›¸å…³ â†’ ç”¨ç›®æ ‡ç½‘ç»œç¨³å®šå­¦ä¹  â†’ ç”¨ Îµ-greedy å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ â†’ ç”¨ MSE æŸå¤±é€¼è¿‘æœ€ä¼˜ Q å‡½æ•°**
>
> å®ƒä¸ä»…æ˜¯ç®—æ³•çš„å¿ å®è¿˜åŸï¼Œæ›´æ˜¯å·¥ç¨‹å®è·µçš„å…¸èŒƒã€‚

---
