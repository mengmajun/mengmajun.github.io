# 第四课：从价值到策略——揭开策略梯度的神秘面纱

在前面三节课中，我们学会了 Q-learning 和 DQN 如何通过“评估动作价值”来间接指导决策。

但有没有一种方式，可以让智能体不再问：“哪个动作得分最高？”  
而是直接回答：“我现在就该这么做！”？

这正是本节课的主题：**策略梯度（Policy Gradient）方法**。

它不依赖查表或值函数，而是让智能体像一位有主见的演员，**直接学会“怎么做最好”**。

> 🎯 本节课核心问题：
>
> - 什么是 policy-based 方法？它和 value-based 有何本质区别？
> - 策略梯度是如何用“梯度上升”来优化策略的？
> - 它如何解决连续动作空间、感知别名等难题？
> - “目标函数的导数怎么算？”——策略梯度定理到底说了什么？

让我们一步步走进这场关于“行为艺术”的数学之旅。

---

## 1. 三大流派对比：Value-Based vs Policy-Based vs Actor-Critic

强化学习有三大思想流派，它们代表了三种不同的“决策哲学”。

### 1.1 Value-Based 方法：先评分，再行动

代表算法：Q-learning, DQN

🧠 核心思想：
> “我不告诉你该做什么，但我告诉你每个动作值多少分。”

- 学习一个价值函数 $Q(s,a)$
- 决策时选择分数最高的动作（如 ε-greedy）
- 策略是“隐式的”——由价值函数推导而来

📌 类比：考试前老师只公布每道题的难度系数，你自己决定先做哪一题。

✅ 优点：收敛快、适合离散动作  
❌ 缺点：难以处理连续动作；存在高估偏差

---

### 1.2 Policy-Based 方法：我有自己的主张

代表算法：REINFORCE, PPO

🧠 核心思想：
> “我不需要打分系统，我知道在这种情况下该怎么做。”  

- 直接学习一个参数化策略 $\pi_\theta(a|s)$
- 输出是一个概率分布，表示在状态 $s$ 下采取每个动作的可能性
- 不依赖价值函数，策略本身就是主角

📌 类比：一个经验丰富的司机不需要思考“左转值8分，右转值6分”，他凭直觉就知道该往哪开。

✅ 优点：
- 可输出随机策略，天然支持探索
- 能处理连续动作空间（如方向盘角度）
- 解决“感知别名”问题（见下文）

❌ 缺点：
- 收敛慢、方差大
- 容易陷入局部最优

---

### 1.3 Actor-Critic 方法：左手价值，右手策略

代表算法：A2C, A3C, PPO

🧠 核心思想：
> “我有两个大脑：一个负责判断局势（Critic），一个负责做出决定（Actor）。”

- 结合前两者之长
- Actor：学习策略 $\pi_\theta$
- Critic：学习价值函数 $V(s)$ 或 $Q(s,a)$，用于评估 Actor 的表现

📌 类比：足球场上，球员（Actor）踢球，教练（Critic）点评：“刚才那脚射门时机不对。”

🎯 这是最主流的现代 RL 架构之一。

---

## 2. 策略梯度：Policy-Based 的核心引擎

### 2.1 Policy-Based 与 Policy Gradient 的关系

你可能会疑惑：

> “Policy-Based 和 Policy Gradient 是不是一回事？”

答案是：  
✅ **Policy Gradient 是 Policy-Based 的一个子集**，但它是最重要的一支。

| 方法 | 是否使用梯度 | 如何优化 |
|------|----------------|-----------|
| 一般 Policy-Based | 否 | 使用进化算法、爬山法等黑箱搜索 |
| **Policy Gradient** | ✅ 是 | 使用梯度上升，沿着性能提升最快的方向更新 |

🧠 打个比方：
- 普通 Policy-Based 就像盲人摸象，试错找路
- 策略梯度则像拿着 GPS 的登山者，知道“往哪个方向走坡最陡”

所以当我们说“策略梯度”，其实是在说：
> “我们要对策略的表现进行求导，找到让它变得更好的方向。”

---

## 3. 策略梯度的优点与缺点

### 3.1 优点一：集成简单，无需额外结构

传统方法要维护 Q 表、目标网络、经验回放……结构复杂。

而策略梯度只需一个神经网络：
- 输入：状态 $s$
- 输出：动作的概率分布 $\pi(a|s; \theta)$

然后根据实际回报调整参数 $\theta$，整个流程干净利落。

🧠 **哲学视角**：  
大道至简。就像禅宗所说：“不立文字，直指人心。”  
策略梯度不做中间推理，直接从经历中学习行为本身。

---

### 3.2 优点二：能学习随机策略，探索内建其中

还记得 ε-greedy 吗？它是人为设定的探索机制。

而在策略梯度中，**探索是内生的**。

例如，在某个状态下，策略可能输出：
```python
π(a|s) = [0.3, 0.5, 0.2]  # 动作0:30%, 动作1:50%, 动作2:20%
```
这意味着智能体会以一定概率尝试非最优动作——这就是探索！

随着训练深入，好的动作概率越来越高，坏的动作逐渐被淘汰。

🧠 **生活类比**  
这就像一个人的性格成长：
- 年轻时犹豫不定，多种选择都试试
- 经历多了，自信增强，主见明确
- 最终形成稳定的行为模式

> ✅ 探索不是“硬编码”的技巧，而是策略演化的一部分。

---

### 3.3 优点三：解决“感知别名”问题

#### ❓ 什么是感知别名（Perceptual Aliasing）？

想象这样一个迷宫：

```
状态A：左边墙，右边墙，前方开阔
状态B：左边墙，右边墙，前方开阔
```

看起来完全一样，但：
- 在状态A，应该向左走才能通向宝藏
- 在状态B，应该向右走才安全，否则会掉坑

如果只看当前画面，AI 无法区分这两个状态，就会犯错。

这就是“感知别名”：**不同的真实状态，在观察上却是一样的**。

🧠 **生活类比**  
这就像两个人长得一模一样：
- 你在餐厅看到一个背影很像你朋友的人
- 你以为是他，结果叫错了名字
- 因为你只看了“表面特征”，没注意上下文

---

#### ✅ 策略梯度如何解决这个问题？

因为它输出的是**随机策略**！

即使两个状态看起来相同，只要训练过程中发现“有时左走得分高，有时右走得分高”，策略就会保持两种动作的概率，而不是死板地选一个。

久而久之，结合记忆或其他机制（如 LSTM），它甚至能学会通过历史信息区分真假状态。

> 💡 这就像人类说：“虽然这俩人长得像，但我记得上次他在咖啡厅，这次在健身房，应该是另一个人。”

---

### 3.4 优点四：适用于连续动作空间

这是策略梯度最大的实用优势。

#### ❓ 为什么 DQN 处理不了连续动作？

比如自动驾驶：
- 方向盘可转动任意角度：15°, 17.3°, 19.8°...
- 动作空间无限多

DQN 需要为每一个可能动作计算 Q 值，然后取最大值——这在数学上不可行。

📌 更严重的是：**最大化连续函数本身就是一个优化问题**！

---

#### ✅ 策略梯度怎么做？

它不比较所有动作，而是直接输出一个**动作分布**，比如：

- 正态分布：$\mathcal{N}(\mu, \sigma)$
- 其中 $\mu$ 和 $\sigma$ 由神经网络输出

然后从中采样一个动作执行。

例如：
```python
μ = 18.5°   # 推荐打方向盘18.5度
σ = 2.0°    # 允许一些扰动
action = sample from N(μ, σ)
```

这样既灵活又高效。

🧠 **比喻升华**  
> DQN 像是在一堆钥匙中找唯一能开门的那一把 —— 无穷把钥匙，怎么找？  
> 策略梯度则是配一把万能钥匙，每次微调形状，越试越接近正确齿形。

---

### 3.5 优点五：更好的收敛特性

#### ❓ 为什么基于值的方法容易震荡？

因为在 Q-learning 中，我们总是取：
$$
\max_a Q(s', a)
$$
这个操作非常“激进”——哪怕 Q 值发生一点点变化，就可能导致选择的动作剧烈切换。

📌 举个例子：
- 昨天你觉得“投资股票更好”
- 今天分析师一句话让你觉得“买房更稳”
- 你就立刻全仓转换，市场一波动又换回来
- 结果来回折腾，收益为负

这就是所谓的“**高方差更新**”或“**策略抖动**”。

---

#### ✅ 策略梯度的优势

它不会突然跳变，而是缓慢调整动作的概率。

- 今天买股的概率是 60%
- 明天变成 58% 或 62%，小幅变化
- 渐进式演进，更加平滑稳定

🧠 **哲学启示**  
> “天下之至柔，驰骋天下之至坚。”  
> 策略梯度不像 Q-learning 那样“斩钉截铁”，而是“润物细无声”，反而更容易逼近最优解。

---

### 3.6 缺点：慢、偏、方差大

尽管优点众多，策略梯度也有明显短板：

| 缺点 | 解释 | 如何缓解 |
|------|------|----------|
| **收敛到局部最优** | 梯度法只能找到最近的高峰，未必是最高山峰 | 多次初始化、引入熵正则项 |
| **训练缓慢** | 每次更新都要等待回合结束，样本利用率低 | 使用 Actor-Critic 加速 |
| **高方差** | 回报波动大 → 梯度噪声大 → 学习不稳定 | 使用 Baseline（如 $V(s)$）降低方差 |

我们在后续 Actor-Critic 课程中将详细讲解这些解决方案。

---

## 4. 深入策略梯度：从直觉到数学

### 4.1 总体思路：让好行为更常出现

策略梯度的核心思想极其朴素：

> 如果某段经历带来了高回报，那么在这段经历中采取的所有动作，都应该被鼓励；
> 如果回报很低，那些动作就应该被抑制。

![策略梯度简化版](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_bigpicture.jpg)

这就像父母教育孩子：
- 考了满分 → “你最近的学习方法很好，继续保持！”
- 考砸了 → “是不是玩游戏太多？以后少玩点”

只不过在这里，“表扬”是通过**增加采取某些动作的概率**来实现的。

---

### 4.2 随机策略：动作偏好的表达方式

我们假设策略是一个参数化的随机函数 $\pi_\theta(a|s)$，通常用神经网络实现。

![随机策略](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png)

输入一个状态，输出每个动作的概率。

这些概率也称为“**动作偏好（Action Preferences）**”——反映了智能体对不同动作的倾向性。

我们的目标就是不断调整 $\theta$，使得：
> “导致高回报的动作偏好越来越高，低回报的越来越低。”

---

### 4.3 目标函数：我们要最大化什么？

我们需要一个衡量策略好坏的指标。

这个指标就是**预期累计回报**，记作：
$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ R(\tau) \right]
$$

展开来看：
$$
J(\theta) = \sum_{\tau} P(\tau; \theta) \cdot R(\tau)
$$

其中：
- $\tau$：一条轨迹（状态-动作序列）
- $R(\tau)$：这条轨迹的总回报
- $P(\tau; \theta)$：在策略 $\pi_\theta$ 下产生该轨迹的概率


![目标函数等于每个轨迹的回报乘以轨迹的概率然后相加](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/expected_reward.png)

🎯 **我们的目标是**：
$$
\max_\theta J(\theta)
$$

即：找到一组参数 $\theta$，使得平均来看，智能体获得的回报最大。

---

## 5. 策略梯度定理：通往可微世界的桥梁

### 5.1 问题来了：怎么求导？

我们要对 $J(\theta)$ 做梯度上升，就需要计算：
$$
\nabla_\theta J(\theta)
$$

但直接求导有两个致命困难：

#### ❌ 困难一：轨迹太多，无法穷举

$J(\theta) = \sum_\tau P(\tau;\theta) R(\tau)$  
轨迹数量指数级增长，根本算不完。

✅ 解法：**蒙特卡洛估计**
- 实际运行几条轨迹
- 用样本均值近似期望值

#### ❌ 困难二：状态转移动态未知且不可导

$P(\tau;\theta)$ 依赖于环境的状态转移概率 $P(s'|s,a)$，但我们往往不知道它，也无法对其求导。

这就像你想优化一辆车的油耗，但不知道发动机内部工作原理。

---

### 5.2 策略梯度定理

幸运的是，**Sutton 等人证明**：

> 即使我们不知道环境动态，也能对 $J(\theta)$ 求导！

这就是的 **策略梯度定理（Policy Gradient Theorem）**：


**目标函数**

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)] = \sum_{\tau} P(\tau; \theta) R(\tau)
$$


我们展开轨迹概率 $P(\tau; \theta)$ 采样自 $\pi_\theta$

$$
P(\tau; \theta) = \mu(s_0) \prod_{t=0}^{H} P(s_{t+1} | s_t, a_t) \pi_\theta(a_t | s_t)
$$

这是**一条轨迹（trajectory）出现的概率**，也就是智能体从开始到结束走完一整条路径的可能性有多大。


想象一个智能体玩游戏：
- 它一开始在状态 $s_0$
- 选择动作 $a_0$
- 环境反馈奖励 $r_1$，并转移到新状态 $s_1$
- 再选动作 $a_1$，得到 $r_2$，进入 $s_2$
- ……
- 直到游戏结束（比如死亡或通关）

这一连串经历就是一个**轨迹（trajectory）**：

$$
\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)
$$

> 💡 注意：我们这里关心的是 **状态和动作序列** 的联合概率，因为奖励是由它们决定的。


那么要让这条轨迹发生，必须满足一系列事件同时成立：

1. 游戏开始时，系统把智能体放在了 $s_0$ —— 这个由环境初始分布 $\mu(s_0)$ 决定  
2. 在 $s_0$，智能体选择了动作 $a_0$ —— 这由策略 $\pi_\theta(a_0|s_0)$ 决定  
3. 环境根据物理规则，从 $(s_0, a_0)$ 转移到了 $s_1$ —— 概率为 $P(s_1|s_0,a_0)$  
4. 在 $s_1$，智能体选择了 $a_1$ —— 概率 $\pi_\theta(a_1|s_1)$  
5. 环境转到 $s_2$ —— 概率 $P(s_2|s_1,a_1)$  
6. ……如此继续，直到最后

所以，**整条轨迹发生的总概率**就是所有这些独立步骤概率的乘积！

这就是那个公式的来源：

$$
P(\tau; \theta) = \underbrace{\mu(s_0)}_{\text{起始状态}} \cdot \prod_{t=0}^{H} \underbrace{P(s_{t+1}|s_t,a_t)}_{\text{环境转移}} \cdot \underbrace{\pi_\theta(a_t|s_t)}_{\text{策略选择动作}}
$$



现在我们计算目标的梯度:

$$
\nabla_\theta J(\theta) = \nabla_\theta \sum_{\tau} P(\tau; \theta) R(\tau)
$$

因为和的梯度等于梯度的和，我们可以重写，对每一项求梯度:

$$
= \sum_{\tau} \nabla_\theta \left[ P(\tau; \theta) R(\tau) \right]
$$

因为 $R(\tau)$ 不依赖 $\theta$, 在微分的时候看作常数项:

$$
= \sum_{\tau} \nabla_\theta P(\tau; \theta) \cdot R(\tau)
$$

现在每一项乘以和除以 $P(\tau; \theta)$ (等价于乘以 1):

$$
= \sum_{\tau} P(\tau; \theta) \cdot \frac{\nabla_\theta P(\tau; \theta)}{P(\tau; \theta)} \cdot R(\tau)
$$

使用 *对数求导技巧* :

$$
\nabla_x \log f(x) = \frac{\nabla_x f(x)}{f(x)}
$$

应用:

$$
\frac{\nabla_\theta P(\tau; \theta)}{P(\tau; \theta)} = \nabla_\theta \log P(\tau; \theta)
$$

因此梯度为:

$$
\nabla_\theta J(\theta) = \sum_{\tau} P(\tau; \theta) \cdot \nabla_\theta \log P(\tau; \theta) \cdot R(\tau)
$$

这个表达式允许我们通过采样来估计梯度，具体来说，我们可以使用蒙特卡洛采样估计来估计梯度

现在注意：左边 $\sum_{\tau} P(\tau; \theta)$ 是一个期望形式, 因此可以写成：

$$
\boxed{
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log P(\tau;\theta) \cdot R(\tau) \right]
}
$$

或者：

$$
\nabla_\theta J(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta \log P(\tau^{(i)}; \theta) \cdot R(\tau^{(i)})
$$

其中每一个 $\tau^{(i)}$ 是在策略 $\pi_\theta$ 下采样的轨迹.

进一步简化 $\nabla_\theta \log P(\tau^{(i)}; \theta)$.

回忆轨迹概率的完整形式:

$$
P(\tau^{(i)}; \theta) = \mu(s_0) \prod_{t=0}^{H} P(s_{t+1}^{(i)} | s_t^{(i)}, a_t^{(i)}) \pi_\theta(a_t^{(i)} | s_t^{(i)})
$$

取对数:

$$
\log P(\tau^{(i)}; \theta) = \log \mu(s_0) + \sum_{t=0}^{H} \log P(s_{t+1}^{(i)} | s_t^{(i)}, a_t^{(i)}) + \sum_{t=0}^{H} \log \pi_\theta(a_t^{(i)} | s_t^{(i)})
$$

计算相关 $\theta$ 的梯度:

$$
\nabla_\theta \log P(\tau^{(i)}; \theta) = \nabla_\theta \log \mu(s_0) + \sum_{t=0}^{H} \nabla_\theta \log P(s_{t+1}^{(i)} | s_t^{(i)}, a_t^{(i)}) + \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)} | s_t^{(i)})
$$

注意初始状态 $\mu(s_0)$ 和状态转移 $P(s_{t+1}|s_t,a_t)$ 和环境有关不依赖 $\theta$. 因此:

$$
\nabla_\theta \log \mu(s_0) = 0, \quad \nabla_\theta \log P(s_{t+1}^{(i)} | s_t^{(i)}, a_t^{(i)}) = 0
$$

所以简化为:

$$
\nabla_\theta \log P(\tau^{(i)}; \theta) = \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)} | s_t^{(i)})
$$

带入到我们的梯度表达式中:

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t | s_t) R(\tau) \right]
$$

使用 $m$ 个采样轨迹来估计:

$$
\boxed{
\nabla_{\theta} J(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \left( \sum_{t=0}^{H} \nabla_\theta \log \pi_\theta(a_t^{(i)} | s_t^{(i)}) \right) R(\tau^{(i)})
}
$$

其中：$R(\tau^{(i)})$ 表示每个轨迹i上的预期回报

策略梯度公式说明：

> 我们不需要对环境进行建模，直接从经验中学习：“如果你完成了一次成功的旅程（高回报 $R(\tau)$），那么你在旅途中做的每一个决策（每个 $\log \pi_\theta(a_t|s_t)$）都应该被鼓励。”

> 表明我们更可以通过和环境互动，记录轨迹来估计期望回报的梯度。

---

### 5.4 实际使用的近似形式（REINFORCE 算法）

由于无法获取真实的 $Q^{\pi}(s,a)$，我们用整条轨迹的回报 $R_t$ 来代替：

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_multiple.png)

伪代码：

注意，我们在计算整个轨迹的回报时候，是从每个时间步t开始计算的，因为从时间步t开始，如果后面的回报是正的，那么从时间步t开始的动作都应该被鼓励

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png)

其中：T表示每个轨迹的每个时间步

这就是经典的 **REINFORCE 算法** 更新规则。

> 🔁 每次成功通关后，回顾全过程，给每个动作“打分”，然后调整策略。

---

# 📚 术语速查表（Glossary）

| 术语 | 含义 |
|------|------|
| **Value-Based** | 通过学习价值函数间接得到策略 |
| **Policy-Based** | 直接学习策略函数 |
| **Policy Gradient** | 使用梯度上升优化策略的方法 |
| **Actor-Critic** | 同时学习策略（Actor）和价值（Critic）的混合架构 |
| **Perceptual Aliasing** | 不同状态观测相同，导致误判 |
| **随机策略** | 输出动作概率分布的策略 |
| **目标函数 $J(\theta)$** | 策略的预期累计回报 |
| **策略梯度定理** | 提供了一种无需环境模型即可计算梯度的方法 |
| **REINFORCE** | 最基础的策略梯度算法，使用蒙特卡洛回报 |

---

# ✅ 本节课小结

今天我们完成了从“价值驱动”到“策略驱动”的跃迁：

1. 理解了三大流派的本质差异：**评分行事 vs 主动决策 vs 双脑协同**
2. 掌握了策略梯度的核心理念：**让带来高回报的行为更常发生**
3. 深入剖析其五大优势：
   - 内建探索
   - 解决感知别名
   - 支持连续动作
   - 收敛平稳
   - 结构简洁
4. 揭秘了策略梯度定理：
   > “即便不知世界如何运转，也能朝着更好前进。”

🧠 **一句话总结**：  
> 策略梯度教会我们一个深刻的道理：  
> **不要总是分析对错，而要学会奖励正确的尝试。**

正如托马斯·爱迪生所说：“我没有失败，我只是找到了一万种不行的方法。”  
策略梯度正是这样一位永不气馁的探索者。

---
