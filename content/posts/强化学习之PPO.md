+++ 
date = '2025-07-04' 
draft = false 
title = '强化学习之PPO' 
categories = ['强化学习'] 
tags = ['PPO'] 
+++


[PPO：近端策略优化](https://arxiv.org/abs/1707.06347)


### **1. 目标函数的直观理解**
PPO的目标是让智能体的策略（即动作选择方式）**越来越好**，但**不能一次改得太猛**。它的目标函数分为两部分：

1. **鼓励好的更新**：如果某个动作能带来更高的奖励（优势值 $A_t > 0$），就增加选择它的概率。
2. **限制更新幅度**：通过`clip`函数防止策略概率比（$r_t(\theta)$）偏离太大，避免“翻车”。

---

### **2. 目标函数拆解**
公式如下：

$$
L\^{CLIP}(\theta) = \mathbb{E}\_t \left[ \min \left( \underbrace{r\_t(\theta) A\_t}_{\text{原始目标}}, \underbrace{\text{clip}(r\_t(\theta), 1-\epsilon, 1+\epsilon) A\_t}_{\text{截断后目标}} \right) \right]
$$


#### **（1）核心变量解释**
- **$ r_t(\theta) = \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} $**  
  - 新旧策略的概率比。比如：
    - 旧策略下某动作概率=0.4，新策略下变为0.6 → $ r_t(\theta) = 0.6/0.4 = 1.5 $。
    - 若新策略概率降低到0.2 → $ r_t(\theta) = 0.2/0.4 = 0.5 $。

- **$ A_t $**（优势函数）  
  - 表示当前动作比平均动作好多少：
    - $ A_t > 0 $：动作比平均水平好，应该鼓励。
    - $ A_t < 0 $：动作比平均水平差，应该抑制。

#### **（2）Min函数的作用**
- **情况1**：如果 $ A_t > 0 $（动作好）：
  - 原始目标 $ r_t(\theta)A_t $ 会鼓励增加动作概率。
  - 但若 $ r_t(\theta) $ 太大（比如 >1+ϵ），则用截断目标 $ (1+\epsilon)A_t $ 限制增幅。
  - **最终取两者较小值** → 避免过度增加概率。

- **情况2**：如果 $ A_t < 0 $（动作差）：
  - 原始目标 $ r_t(\theta)A_t $ 会抑制动作概率。
  - 但若 $ r_t(\theta) $ 太小（比如 <1-ϵ），则用截断目标 $ (1-\epsilon)A_t $ 限制降幅。
  - **最终取两者较小值** → 避免过度减少概率。

#### **（3）Clip函数的直观表现**
- **限制范围**：$ r_t(\theta) $ 被限制在 $[1-\epsilon, 1+\epsilon]$ 之间（例如$\epsilon=0.2$ → [0.8, 1.2]）。
- **效果**：
  - 如果 $ r_t(\theta)=1.5 $（想增加50%概率），但$\epsilon=0.2$ → 最多允许1.2倍（即20%增幅）。
  - 如果 $ r_t(\theta)=0.3 $（想减少70%概率），但$\epsilon=0.2$ → 最多允许0.8倍（即20%降幅）。

---

### **3. 为什么这样设计？**
- **传统策略梯度的问题**：直接最大化 $ r_t(\theta)A_t $ 会导致策略突变，可能彻底抛弃某些动作。
- **PPO的解决方案**：通过clip机制限制更新幅度，实现**小幅多次的稳定更新**。

---
### **4. “近端”的数学含义**
- **“Proximal”** 直译为“邻近的”，在优化问题中指的是**对参数更新的约束**，确保每次迭代后的新策略与旧策略保持在“邻近区域”内。
- 在PPO中，这个约束通过两种方式实现：
  1. **Clip机制**（硬约束）：直接截断概率比 $ r_t(\theta) $ 的范围。
  2. **KL散度惩罚**（软约束，PPO-Penalty变种）：在目标函数中添加KL散度项。

PPO-Clip是更常用的形式，因此“近端”主要指clip操作。

---

### **5. 为什么“近端”能稳定训练？**
- **避免策略崩溃**：传统策略梯度中，过大的策略更新可能导致某些动作概率归零（无法恢复）。
- **信任区域（Trust Region）**：PPO的clip隐式定义了策略更新的信任区域，确保新策略的性能不会剧烈波动。类似TRPO的显式约束，但计算更简单。

---

### **6. 举个具体例子**
假设：
- 旧策略概率 $ \pi_{\text{old}}(a|s) = 0.5 $，
- 新策略概率 $ \pi_{\theta}(a|s) = 0.7 $ → $ r_t(\theta) = 0.7/0.5 = 1.4 $，
- 优势值 $ A_t = 0.5 $（动作较好），
- 超参数 $ \epsilon = 0.2 $。

计算：
1. 原始目标：$ 1.4 \times 0.5 = 0.7 $。
2. 截断目标：$ \text{clip}(1.4, 0.8, 1.2) \times 0.5 = 1.2 \times 0.5 = 0.6 $。
3. 取最小值：$ \min(0.7, 0.6) = 0.6 $。

**结果**：虽然原始目标想让损失达到0.7，但clip后限制为0.6，避免策略更新过大。

---

### **7. 代码实现（PyTorch示例）**
```python
def ppo_loss(new_probs, old_probs, advantages, epsilon=0.2):
    ratio = new_probs / old_probs  # r_t(θ)
    clipped_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)
    loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()
    return loss
```

---

### **总结**
PPO的目标函数本质是：
1. **利用优势函数** $ A_t $ 判断动作好坏。
2. **通过概率比** $ r_t(\theta) $ 调整策略。
3. **用clip限制更新幅度**，避免“步子太大扯着蛋”。

这种设计让PPO成为**既高效又稳定**的强化学习算法。

---

参考资料

- [【论文解读】PPO：近端策略优化算法（Proximal Policy Optimization Algorithms）](https://zhuanlan.zhihu.com/p/9914683156)
- [动手学强化学习](https://hrl.boyuai.com/chapter/2/ppo%E7%AE%97%E6%B3%95)
