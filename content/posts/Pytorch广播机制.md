+++ 
date = '2025-06-12' 
draft = false 
title = 'Pytorch广播机制' 
categories = ['Pytorch'] 
tags = ['Pytorch'] 
+++

## 基本概念

广播是PyTorch和NumPy中一种智能的维度扩展机制，允许不同形状的张量进行运算。


## 广播规则

1. **维度对齐**：从最右边开始逐维比较
2. **兼容条件**：
   - 维度相等 ✅
   - 其中一个为1 ✅
3. **扩展规则**：大小为1的维度会被复制扩展
4. **不兼容情况**：其他所有情况都会报错 ❌

---

## 编码器的mask广播计算

编码器中需要屏蔽掉填充token，避免它们参与注意力权重计算，假设：

- 注意力机制中Q和K_T点积相乘后的维度大小是 (2,2,5,5)，维度分别为[batch_size,head_num,seq_len,seq_len], 也就是2个句子序列，2个头，每个句子序列的长度为5
- 假设第1个句子后面2个是填充token，第2个句子后面1个是填充token

```python
import torch
scaled_matmul = torch.randint(10, 100, (2, 2, 5, 5))  # batch_size head_num seq_len seq_len

tensor([[[[83, 31, 34, 65, 28],
          [24, 86, 81, 58, 26],
          [36, 66, 87, 49, 50],
          [23, 39, 40, 16, 79],
          [80, 58, 47, 65, 73]],

         [[87, 64, 14, 94, 89],
          [14, 27, 66, 11, 49],
          [23, 45, 93, 25, 86],
          [97, 58, 17, 56, 50],
          [34, 32, 85, 62, 43]]],


        [[[14, 65, 52, 51, 26],
          [30, 16, 60, 40, 75],
          [44, 98, 54, 27, 39],
          [54, 66, 59, 72, 16],
          [29, 74, 76, 75, 80]],

         [[82, 96, 46, 78, 63],
          [46, 62, 19, 12, 77],
          [72, 87, 15, 49, 93],
          [63, 27, 13, 41, 55],
          [38, 73, 20, 44, 95]]]])



# 制作掩码
src = torch.ones(2, 5)

mask = src > 0

# 第1个句子后面2个是填充token，第2个句子后面1个是填充token
mask[0][3:] = False
mask[1][4:] = False

mask

tensor([[ True,  True,  True, False, False],
        [ True,  True,  True,  True, False]])

# 将维度数量整理成一致
mask = mask.unsqueeze(1).unsqueeze(2)

mask.shape

torch.Size([2, 1, 1, 5])

# mask计算，将mask为False的位置填充为极小值
scaled_matmul_mask = scaled_matmul.masked_fill(mask == False, float(1e-20))

scaled_matmul_mask
tensor(
    # 这是第一个句子的2个头
        [[[[83, 31, 34,  0,  0],
          [24, 86, 81,  0,  0],
          [36, 66, 87,  0,  0],
          [23, 39, 40,  0,  0],
          [80, 58, 47,  0,  0]],

         [[87, 64, 14,  0,  0],
          [14, 27, 66,  0,  0],
          [23, 45, 93,  0,  0],
          [97, 58, 17,  0,  0],
          [34, 32, 85,  0,  0]]],


    # 这是第二个句子的2个头
        [[[14, 65, 52, 51,  0],
          [30, 16, 60, 40,  0],
          [44, 98, 54, 27,  0],
          [54, 66, 59, 72,  0],
          [29, 74, 76, 75,  0]],

         [[82, 96, 46, 78,  0],
          [46, 62, 19, 12,  0],
          [72, 87, 15, 49,  0],
          [63, 27, 13, 41,  0],
          [38, 73, 20, 44,  0]]]])
```

#### 广播步骤解析：

1. `mask` 是一个布尔张量，shape 也是 `[2, 1, 1, 5]`
2. `scaled_matmul` 的 shape 是 `[2, 2, 5, 5]`
3. 根据广播规则：
   - 最右边的维度都是 2 ✅
   - 再往左是 `1` vs `2`，允许广播 ✅
   - 再往左是 `1` vs `5`，允许广播 ✅
   - 最左边是 `5` vs `5`，匹配 ✅

因此，`mask` 被广播成 `[2, 2, 5, 5]`，正好和 `scaled_matmul` 对应上。

---

#### 🧮 结果解释

- 每个 head、每个 query token 都会看到 key 的所有位置。
- mask 只关心 key 的哪些位置是 padding。
- 所以我们只需要提供一个 `[B, 1, 1, T_k]` 的 mask，它会在 head 和 query 维度上广播。
- 这样就能正确地屏蔽掉所有头、所有 query 对 padding key 的关注。

## 解码器的mask广播计算

解码器需要屏屏蔽掉当前token未来位置

```python
# 制作每个句子都会用到的下三角矩阵，下三角为1，上三角为0

mask = torch.tril(torch.ones(5,5))

mask

tensor([[1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1.]])

mask = mask.expand(2, 1, 5, 5)

mask.shape

torch.Size([2, 1, 5, 5])

# 开始计算mask
scaled_matmul_mask = scaled_matmul.masked_fill(mask == 0, float(1e-20))

scaled_matmul_mask

# 可以看到当前token只能看到之前token了
tensor([[[[83,  0,  0,  0,  0],
          [24, 86,  0,  0,  0],
          [36, 66, 87,  0,  0],
          [23, 39, 40, 16,  0],
          [80, 58, 47, 65, 73]],

         [[87,  0,  0,  0,  0],
          [14, 27,  0,  0,  0],
          [23, 45, 93,  0,  0],
          [97, 58, 17, 56,  0],
          [34, 32, 85, 62, 43]]],


        [[[14,  0,  0,  0,  0],
          [30, 16,  0,  0,  0],
          [44, 98, 54,  0,  0],
          [54, 66, 59, 72,  0],
          [29, 74, 76, 75, 80]],

         [[82,  0,  0,  0,  0],
          [46, 62,  0,  0,  0],
          [72, 87, 15,  0,  0],
          [63, 27, 13, 41,  0],
          [38, 73, 20, 44, 95]]]])

```
