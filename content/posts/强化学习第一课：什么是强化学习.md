
## 1. 宏观概念

强化学习（Reinforcement Learning, RL）是一种让智能体（Agent）通过与环境互动来“学会做决策”的机器学习范式。

它的核心思想是：  
> **试错 + 反馈 = 学习**

想象一个孩子第一次学走路——他会摇晃着迈出一步，如果没摔倒，就会得到父母的鼓励（正反馈）；如果跌倒了，就会感到疼痛或失望（负反馈）。久而久之，他就学会了如何走得稳。

在强化学习中，这个过程被形式化为：

- 智能体观察环境的状态
- 根据当前状态选择一个动作
- 执行动作后，环境发生变化，并返回一个数值型的**奖励（Reward）**
- 智能体根据奖励调整策略，以便在未来获得更多奖励

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_1.jpg)

> ✅ **直觉理解（新增）**  
你可以把强化学习看作是一场没有剧本的探险游戏：
- 你不知道地图长什么样
- 不知道宝藏在哪，也不知道哪里有陷阱
- 唯一能依靠的是每次行动后的提示：“+10金币” 或 “掉血50”
- 你要靠这些零碎的反馈，慢慢摸索出一条通往胜利的道路

这就是强化学习的本质：**从经验中学习最优行为策略，仅凭奖励信号作为指南针。**

---

## 2. 正式定义

强化学习是一个用于解决**序列决策问题**的数学框架。它通过构建一个能够与环境持续交互的智能体，使其在试错过程中根据接收到的**标量奖励信号**不断改进自身行为，最终学会完成特定目标。

换句话说：  
> 强化学习 = 智能体 + 环境 + 动作 + 奖励 + 学习规则

其目标不是即时满足，而是长期收益的最大化。

---

# 2. 强化学习的学习框架

## 2.1 学习过程：状态 → 动作 → 奖励 → 新状态

强化学习的过程是一个循环：

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg)

每一步都遵循这样的流程：

1. 智能体处于某个**状态 $s_t$**
2. 观察该状态并决定采取某个**动作 $a_t$**
3. 环境响应此动作，转移到新状态 $s_{t+1}$，同时给出一个**奖励 $r_{t+1}$**
4. 智能体基于新的状态和奖励更新自己的“经验”
5. 重复以上步骤……

这个循环不断进行，直到任务结束（如游戏通关或失败），或者无限延续（如股票交易系统）。

### 🎯 智能体的目标：最大化预期回报

智能体并不关心某一次动作是否立刻获得高奖励，而是希望在整个生命周期内获得最多的**累计奖励**，我们称之为“**预期回报（Expected Return）**”。

> 🔍 举个生活中的例子：  
当你考虑换工作时，你会问：“这家公司的年薪是多少？”但这还不够。  
真正影响你决策的是：“未来五年我能赚多少？发展空间有多大？会不会中途裁员？”  
也就是说，你在评估**未来的总收益期望**，而不是今天发不发奖金。

这正是强化学习的思想：  
> **不要只看眼前的小利，要看长远的整体回报。**

---

## 2.2 核心思想：奖励假设（Reward Hypothesis）

为什么我们要最大化预期回报？这是由**奖励假设**决定的：

> 所有目标和目的都可以被看作是**最大化一个累积的数值奖励信号**。

这意味着：
- 无论是下棋赢对手、自动驾驶安全行驶、还是机器人学会走路
- 我们都可以把这些复杂目标转化为一系列“奖励”设计
- 只要奖励设置得当，智能体自然会朝着我们期望的方向演化

🧠 **哲学视角（新增）**  
这就像人生信条：“一切努力终将归结为幸福的总量”。  
虽然现实中我们很难量化“幸福”，但在 RL 中，我们用“奖励”代替它，成为衡量成功的唯一尺度。

⚠️ 注意：奖励的设计至关重要！错误的奖励会导致“作弊式聪明”——比如 AI 发现不停撞墙也能得分，于是它就一直撞墙……

---

### 马尔可夫特性（Markov Property）

强化学习通常建模为**马尔可夫决策过程（MDP）**，其核心是马尔可夫特性：

> **下一时刻的状态只依赖于当前状态和当前动作，与过去的历史无关。**

数学表达为：
$$
P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0,a_0,s_1,a_1,\dots,s_t,a_t)
$$

✅ **直觉理解（新增）**  
你可以把它理解为一种“活在当下”的智慧。

> 就像登山者不需要记住他之前踩过哪块石头，只要知道现在站在哪里、眼前有哪些路可走，就能做出下一步的选择。

💡 类比思考：
- 如果你是失忆的人，醒来发现自己在一个山腰上，面前有三条小路
- 虽然你不记得是怎么来的，但只要看清当前位置的地势、天气、体力，依然可以判断哪条路最可能通向山顶
- 这就是马尔可夫性的精髓：**当前状态包含了所有必要信息**

当然，在真实世界中，并非所有任务都满足马尔可夫性。这时我们需要设计更好的“状态表示”，例如使用记忆机制（如 LSTM）或将历史观测打包成状态输入。

---

## 2.3 观察与状态空间

| 名称 | 含义 | 示例 |
|------|------|-------|
| **状态（State）** | 环境的完整描述，包含所有必要信息 | 围棋棋盘上所有棋子的位置 |
| **观察（Observation）** | 智能体实际感知到的部分信息 | 游戏画面像素、传感器数据 |

📌 关键区别：
- 在完全可观测环境中（如围棋），观察 ≈ 状态
- 在部分可观测环境中（如第一人称游戏），观察 ≠ 状态（你看不到背后的敌人）

因此，很多现实问题属于**部分可观测马尔可夫决策过程（POMDP）**，需要智能体具备一定的“推理”能力。

---

## 2.4 动作空间

动作空间是指智能体可以在环境中执行的所有可能动作的集合。

分为两类：

| 类型 | 特点 | 示例 |
|------|------|--------|
| **离散动作空间** | 动作数量有限且明确 | 上/下/左/右、出拳/格挡 |
| **连续动作空间** | 动作是实数范围内的值 | 油门力度（0~1）、方向盘角度（-30°~+30°） |

🧠 **直觉理解（新增）**  
这就像生活中做选择：
- 点外卖时选“米饭”还是“面条” → 离散选择
- 调空调温度调到23.5℃ → 连续调节

不同任务需要不同的算法支持。例如 DQN 适用于离散动作，而 DDPG/PPO 更适合连续控制。

---

## 2.5 奖励与折扣机制

### 奖励：唯一的老师

在强化学习中，**奖励是智能体唯一的外部反馈**，相当于老师给学生的打分。

- 正奖励（+）：鼓励类似行为（吃到了奶酪）
- 负奖励（-）：惩罚不当行为（被猫抓了）

智能体会逐渐学会：
> “哪些动作让我得分多？我要多做；哪些让我扣分？我要避免。”

---

### 折扣回报：关注近期，兼顾长远

理论上，我们可以简单地把所有未来奖励加起来：
$$
R = r_1 + r_2 + r_3 + \cdots + r_T
$$
但这有个问题：**越远的奖励越不确定**。

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg)

比如那只老鼠：
- 吃到近处的奶酪概率很高 → 值得追求
- 冒险去猫旁边的奶酪？万一被吃了，后面的奖励全没了！

所以我们引入**折扣因子 $\gamma$（gamma）**，让未来的奖励随时间衰减：

$$
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
$$

其中 $\gamma \in [0, 1]$，常见取值 0.9 ~ 0.99。

| $\gamma$ 值 | 含义 | 行为倾向 |
|-------------|------|----------|
| 接近 0 | 只看重 immediate reward | 急功近利，短视 |
| 接近 1 | 高度重视长期回报 | 有耐心，愿投资未来 |

🎯 **生活类比（新增）**  
想想你的人生规划：
- 如果你觉得“明天不一定还在”，那你可能会挥霍金钱、及时行乐（$\gamma$ 很小）
- 如果你相信“十年树木百年树人”，你会坚持读书、锻炼身体、积累人脉（$\gamma$ 接近 1）

智能体也是如此：**γ 是它的“时间观”或“人生观”**。

---

# 3. 任务类型

| 类型 | 特征 | 示例 |
|------|------|--------|
| **剧情式任务（Episodic Task）** | 有明确起点和终点，周期性重启 | 游戏关卡、机器人走迷宫 |
| **持续性任务（Continuing Task）** | 没有终止状态，永远运行 | 股票交易、服务器资源调度 |

📌 数学处理差异：
- 剧情任务：累计回报可直接求和
- 持续任务：必须使用折扣回报，否则总奖励趋于无穷

---

# 4. 探索 vs 利用（Exploration vs Exploitation）

这是强化学习中最深刻的**两难抉择**。

| 概念 | 含义 | 类比 |
|------|------|--------|
| **探索（Exploration）** | 尝试新动作，发现未知奖励 | 尝试一家新开的餐厅 |
| **利用（Exploitation）** | 使用已知最优策略获取稳定回报 | 回头吃最喜欢的那家老店 |

❌ 如果只探索：
- 总在尝试，从未精通，收益波动大

❌ 如果只利用：
- 错过更好的机会，陷入局部最优

✅ 最优策略：**先广泛探索，再逐步聚焦利用**

🧠 **生活哲学类比（新增）**  
这就像人生的两个阶段：
- 年轻时多尝试：换城市、换工作、谈恋爱，拓宽视野（探索）
- 成熟后深耕领域：专注事业、经营家庭，稳定产出（利用）

在 RL 中，我们也常用 ε-greedy、Softmax、UCB 等策略来动态平衡两者。

---

# 5. 解决强化学习问题的两种主要方法

## 什么是策略（Policy）？

策略（Policy）是智能体的“行为准则”，告诉它在每个状态下应该采取什么动作。

记作 $\pi(a|s)$，表示在状态 $s$ 下选择动作 $a$ 的概率。

我们的终极目标是找到**最优策略 $\pi^*$**，使得长期回报最大。

---

### 方法一：基于策略的方法（Policy-Based Methods）

直接学习策略函数 $\pi(s) \rightarrow a$

即：**给定状态，输出应采取的动作（或动作的概率分布）**

#### 两种策略类型：

| 类型 | 描述 | 示例 |
|------|------|--------|
| **确定性策略** | 相同状态下总是输出同一动作 | 控制机械臂精确抓取 |
| **随机性策略** | 输出动作的概率分布，便于探索 | 游戏中偶尔随机跳跃以防被预测 |

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_5.jpg)

🔹 **优点**：
- 可处理连续动作空间
- 学习过程更稳定

🔹 **缺点**：
- 收敛速度慢
- 方差较高

👉 典型算法：REINFORCE, PPO

---

### 方法二：基于值的方法（Value-Based Methods）

不直接输出动作，而是学习一个**价值函数**，评估每个状态或状态-动作对的好坏。

然后选择价值最高的动作。

常用函数：
- $V(s)$：状态价值函数 —— 从状态 $s$ 开始能获得的预期回报
- $Q(s,a)$：动作价值函数 —— 在状态 $s$ 执行动作 $a$ 后的预期回报

智能体选择使 $Q(s,a)$ 最大的动作：
$$
a^* = \arg\max_a Q(s,a)
$$

![](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg)

🔹 **优点**：
- 收敛快，效率高
- 易于实现（如 DQN）

🔹 **缺点**：
- 难以处理连续动作空间（需额外技巧如 DDPG）
- 间接学习策略，可能存在偏差

👉 典型算法：Q-Learning, Deep Q-Network (DQN)

---

### 💡 对比总结

| 维度 | 基于策略 | 基于值 |
|------|---------|--------|
| 是否直接学习策略 | 是 | 否 |
| 输出形式 | 动作或概率分布 | 状态/动作的价值 |
| 适合动作空间 | 连续 & 离散 | 主要离散 |
| 探索方式 | 内置（随机策略） | 需手动添加（如 ε-greedy） |
| 典型算法 | REINFORCE, PPO | Q-Learning, DQN |

> ✅ 实际中，许多现代算法（如 A3C、PPO）融合了二者优势，称为**Actor-Critic 架构**。

---

# 6. 深度强化学习：当 RL 遇见深度学习

传统强化学习依赖表格存储（如 Q-table），难以应对高维状态（如图像）。

**深度强化学习（Deep RL）** 将**深度神经网络**引入 RL，用来近似策略函数或价值函数。

例如：
- 输入一张游戏画面（图像）
- 网络输出每个动作的价值或概率
- 智能体据此决策

从此，AI 可以玩 Atari 游戏、下围棋（AlphaGo）、控制机器人……  
这一切都得益于“深度”二字带来的强大表征能力。

> 🔤 名字来源：“深度” = 使用深层神经网络  
“强化学习” = 学习机制  
合称：**深度强化学习**

---

# 📚 术语表（Glossary）

| 术语 | 解释 |
|------|------|
| **Agent（智能体）** | 学习和决策的主体 |
| **Environment（环境）** | Agent 所处的世界，对其动作做出反应 |
| **State（状态）** | 环境的完整信息描述 |
| **Observation（观察）** | Agent 实际感知到的信息（可能不完整） |
| **Action（动作）** | Agent 可执行的操作 |
| &nbsp;&nbsp;– 离散动作 | 如上下左右 |
| &nbsp;&nbsp;– 连续动作 | 如油门大小、转向角 |
| **Reward（奖励）** | 来自环境的标量反馈信号 |
| **Reward Hypothesis（奖励假设）** | 所有目标均可归约为最大化累积奖励 |
| **Discounting（折扣）** | 对未来奖励打折，体现不确定性 |
| &nbsp;&nbsp;– γ（gamma） | 折扣因子，控制远期奖励权重 |
| **Task（任务）** | 需要解决的问题 |
| &nbsp;&nbsp;– 剧情式任务 | 有起止，如游戏关卡 |
| &nbsp;&nbsp;– 连续性任务 | 无终止，如自动交易 |
| **Exploration（探索）** | 尝试新动作以获取更多信息 |
| **Exploitation（利用）** | 使用已有知识获取最大回报 |
| **Policy（策略）** | 决策规则，映射状态到动作 |
| &nbsp;&nbsp;– 确定性策略 | 固定输出动作 |
| &nbsp;&nbsp;– 随机性策略 | 输出动作概率分布 |
| **Policy-Based Method** | 直接学习策略函数 |
| **Value-Based Method** | 学习价值函数，间接指导动作 |
| **Markov Property** | 下一状态仅依赖当前状态和动作 |
| **Deep RL** | 使用深度神经网络的强化学习 |

---

# ✅ 本节课小结

今天我们建立了强化学习的基本世界观：

- 智能体通过**试错+奖励反馈**学习
- 目标是最大化**折扣后的累计回报**
- 遵循**马尔可夫特性**简化决策
- 面临**探索与利用**的经典权衡
- 可采用**基于策略**或**基于值**的方法求解
- 加入深度学习后，能处理复杂任务

🧠 **一句话总结**：  
> 强化学习教会机器像人一样，在不确定的世界中，通过不断尝试和反思，学会做出最好的选择。

---
