
+++ 
date = '2025-06-13' 
draft = false 
title = '一步一步推导交叉熵损失函数' 
categories = ['损失函数'] 
tags = ['损失函数', '交叉熵']
+++

## 一、信息论基础：从概率到信息量

### 1. 概率（Probability）

概率是描述某一事件发生可能性的数值度量，取值范围在[0,1]之间。在深度学习中，模型的输出通常是一个概率分布，表示模型对不同结果的"信心"。例如，在猫狗分类任务中，模型输出[0.7, 0.3]表示其认为输入图片有70%的概率是猫，30%的概率是狗。

### 2. 信息量（Information Content）

信息量衡量的是一个事件发生时所包含的"意外程度"。其核心思想是：越不可能发生的事件，包含的信息量越大；而必然发生的事件，信息量为0。

**数学定义**：
对于一个概率为$p$的事件，其信息量定义为：
$$I(x) = -\log(p(x))$$

**单位说明**：
- 当对数以2为底时，单位为"比特（bit）"
- 当使用自然对数时，单位为"奈特（nat）"

**示例**：
- 抛一枚公平硬币，正面朝上的概率$p=0.5$，其信息量为：
  $$I=-\log_2(0.5)=1 \text{ 比特}$$
- 太阳从东方升起（概率$p\approx1$），其信息量约为：
  $$I\approx-\log_2(1)=0 \text{ 比特}$$

## 二、熵：系统不确定性的度量

### 1. 熵（Entropy）

熵是对一个概率分布中不确定性的整体度量，也就是整个系统的平均信息量，它表示描述该分布所需的平均信息量，也就是每种情况的概率乘以该情况下的信息量。对于离散概率分布$p(x)$，其熵定义为：

$$H(p) = -\sum_{x} p(x) \log p(x)$$

### 2. 熵的直观理解

- 当分布越均匀时，熵越大（不确定性越高）
- 当分布越集中时，熵越小（不确定性越低）

**示例**：
1. **公平硬币**（$p(正面)=p(反面)=0.5$）：
   $$H(p) = -[0.5\log_2(0.5) + 0.5\log_2(0.5)] = 1 \text{ 比特}$$
2. **作弊硬币**（$p(正面)=1, p(反面)=0$）：
   $$H(p) = -[1\log_2(1) + 0\log_2(0)] = 0 \text{ 比特}$$

## 三、交叉熵：衡量分布差异的核心工具

### 1. 交叉熵（Cross-Entropy）

交叉熵用于衡量两个概率分布$p(x)$（真实分布）和$q(x)$（预测分布）之间的差异，其数学表达式为：

$$H(p, q) = -\sum_{x} p(x) \log q(x)$$

### 2. 交叉熵的直观解释

交叉熵表示"用预测分布$q$来描述真实分布$p$时，所需的平均信息量"。其关键性质包括：

- 当$q=p$时，交叉熵等于熵：$H(p, q)=H(p)$
- 当$q$与$p$差异越大时，交叉熵越大
- 交叉熵始终大于等于熵，其差值为KL散度

### 3. 交叉熵与KL散度的关系

KL散度（Kullback-Leibler Divergence）是另一种衡量分布差异的指标，定义为：

$$D_{KL}(p||q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}$$

通过数学推导可得：

$$H(p, q) = H(p) + D_{KL}(p||q)$$

由于$H(p)$是真实分布的固有属性（训练过程中固定不变），因此**最小化交叉熵等价于最小化KL散度**，即让预测分布尽可能接近真实分布。

### 4. 交叉熵的非对称性与实际应用

需要注意的是，交叉熵（及KL散度）是非对称的，即$H(p, q) \neq H(q, p)$。这一特性在实际应用中具有重要意义：

- 当真实分布$p$中某事件概率很高时，预测分布$q$对该事件的低估会导致交叉熵显著增加
- 而对低概率事件的高估，对交叉熵的影响相对较小


## 四、交叉熵损失函数

### 1. 多分类交叉熵（Multiclass Cross-Entropy）

对于一个样本 $ i $，其真实标签是一个 one-hot 向量 $ y_i = [y_1, y_2, ..., y_C] $，其中 $ C $ 是类别总数；模型输出为 logits（未归一化的分数），通过 Softmax 转换为概率分布：

$$
\hat{y}_i = \text{Softmax}(z_i)
$$

则交叉熵损失为：

$$
L = -\sum_{c=1}^C y_{ic} \log(\hat{y}_{ic})
$$

> 其中 $ y_{ic} $ 是第 $ i $ 个样本属于第 $ c $ 类的真实标签（0 或 1），$ \hat{y}_{ic} $ 是模型预测的概率。

由于真实标签是 one-hot 形式，只有对应类别的项是非零的，因此该公式可简化为：

$$
L = -\log(\hat{y}_{i, \text{true}})
$$

也就是说，我们只关心模型对正确类别的预测概率。

---

在大模型中（如 GPT、LLaMA、ChatGLM 等），交叉熵损失函数主要用于以下两类任务：

### 2. 语言建模（Language Modeling）

语言模型的目标是根据前面的上下文预测下一个词（token）。这本质上是一个**多分类任务**：每个时间步要从词表中选择一个词。

模型输入输出结构：

- 输入：一个 token 序列 $ x = (x_1, x_2, ..., x_T) $
- 输出：每个位置输出一个 logit 向量，维度为词表大小 $ V $
- 损失函数：交叉熵损失

数学表达：

$$
\mathcal{L} = -\sum_{t=1}^{T-1} \log P(x_{t+1} \mid x_1, ..., x_t; \theta)
$$

其中：
- $ \theta $ 是模型参数；
- $ P(x_{t+1} \mid \cdot) $ 是模型预测的下一个词的概率分布；
- 只有真实词对应的 log-prob 被保留下来计算损失。

这个损失函数也被称为 **负对数似然（Negative Log-Likelihood, NLL）**。

---

### 3. 序列到序列任务（如翻译、摘要）

在 Transformer 的 Seq2Seq 架构中，解码器每一步都在预测下一个目标 token，同样使用交叉熵损失函数。

损失函数形式

$$
\mathcal{L} = -\sum_{t=1}^{T} \log P(y_t \mid y_1, ..., y_{t-1}, x; \theta)
$$

其中：
- $ x $ 是输入序列（如英文句子）；
- $ y $ 是目标序列（如中文翻译）；
- 每个位置的预测都基于前面的输出和输入。

### 4. 最大似然

**最小化交叉熵损失 ≡ 最大化真实数据序列的似然概率**

也就是说：  
我们在训练语言模型时，使用的交叉熵损失函数，其实就是在让模型尽可能多地“预测出正确的词”。

---

假设我们有一个语言模型，它要根据前面的词来预测下一个词。

比如句子是：
```
The cat sat on the mat.
```

模型的任务就是：
- 给定 "The"，预测下一个词是 "cat"
- 给定 "The cat"，预测下一个词是 "sat"
- ……

这就是一个典型的**自回归语言建模任务（Autoregressive Language Modeling）**。

---

我们的目标是什么？

我们要让模型输出的概率分布，在每个位置都尽可能地给**真实的下一个词**分配更高的概率。

换句话说，我们希望最大化整个句子出现的联合概率：

$$
P(\text{The cat sat on the mat}) = P(\text{The}) \cdot P(\text{cat}|\text{The}) \cdot P(\text{sat}|\text{The cat}) \cdots
$$

但这太复杂了，所以我们通常只关注条件概率：

$$
\max_\theta \sum_{t=1}^T \log P(x_t \mid x_1, ..., x_{t-1}; \theta)
$$

这就是**最大似然估计（Maximum Likelihood Estimation, MLE）**。

---

为什么说这等价于最小化交叉熵？

让我们回到交叉熵的定义：

对于两个分布 $ p $（真实分布）和 $ q $（模型预测分布），交叉熵为：

$$
H(p, q) = -\sum_x p(x) \log q(x)
$$

在语言模型中：

- 真实分布 $ p(x) $ 是 one-hot 分布（因为下一词是确定的）
- 模型预测分布 $ q(x) $ 是 softmax 后的概率分布

所以交叉熵变成：

$$
H(p, q) = -\log q(x_{\text{true}})
$$

也就是负的 log-probability（负对数似然）

---

所以我们可以得到如下结论：

| 目标 | 公式 |
|------|------|
| **最大似然估计（MLE）** | $\max_\theta \sum_t \log P(x_t \mid x_{<t}; \theta)$ |
| **最小化交叉熵损失** | $\min_\theta \sum_t -\log P(x_t \mid x_{<t}; \theta)$ |

它们是一对“正负关系”！

---

**举个例子说明一下** 

假设模型预测下一个词的概率分布如下：

| 词 | 概率 |
|----|------|
| dog | 0.2 |
| cat | 0.5 |
| ball | 0.3 |

如果真实的词是 `cat`，那么它的 log-prob 就是：

$$
\log(0.5) \approx -0.693
$$

此时交叉熵损失就是：

$$
-\log(0.5) = 0.693
$$

如果我们能让模型把 `cat` 的概率提高到 0.8，那 log-prob 就变成：

$$
\log(0.8) \approx -0.223
$$

交叉熵损失就降低为 0.223 —— 更小了！

所以，**让交叉熵损失变小，就是在让 log-prob 变大，也就是让模型更“确信”地预测出正确词**。

---

总结：交叉熵损失与最大似然的关系

| 视角 | 描述 |
|------|------|
| **最大似然估计（MLE）** | 让模型对真实词的预测概率尽可能高 |
| **交叉熵损失** | 衡量模型预测与真实标签之间的差距 |
| **两者关系** | 交叉熵损失 = - log(真实词的概率)，因此最小化交叉熵 ≡ 最大化 log-likelihood |
| **实际意义** | 在训练 GPT、BERT 等语言模型时，使用交叉熵损失就是在做最大似然估计 |

---

为什么要用负号？

因为我们希望最大化 log-prob，而优化器默认是**最小化目标函数**，所以我们在前面加一个负号，变成最小化问题：

$$
\min_\theta -\log P(x_t \mid \cdot; \theta)
$$

这就变成了标准的交叉熵损失形式。

---

## 五、PyTorch 中的交叉熵损失函数实现

在 PyTorch 中，`nn.CrossEntropyLoss()` 是最常用于多分类任务的损失函数。

### 1. 特点

- 输入 `input` 是未经过 softmax 的原始 logits（shape: `[N, C]` 或 `[B, C, T]`）；
- 输入 `target` 是类别索引（shape: `[N]` 或 `[B, T]`），不是 one-hot 编码；
- 内部自动应用了 `log_softmax` 和 `nll_loss`，等价于先做 softmax 再取 log 再计算负对数似然。

### 2. 示例代码

```python
import torch
import torch.nn as nn

# 假设 batch_size = 2，seq_len = 4，vocab_size = 10000
logits = torch.randn(2, 4, 10000)  # [batch_size, seq_len, vocab_size]
labels = torch.randint(0, 10000, (2, 4))  # [batch_size, seq_len]

# 定义交叉熵损失函数
criterion = nn.CrossEntropyLoss(ignore_index=-100)  # ignore_index 可选，用于忽略 padding

# 计算损失（注意要调整 logits 的 shape）
loss = criterion(logits.view(-1, 10000), labels.view(-1))

print("Cross Entropy Loss:", loss.item())
```

> 注意：因为 PyTorch 的 `CrossEntropyLoss` 要求 logits 的 shape 是 `[N, C]`，所以我们要把 `[B, T, V]` reshape 成 `[B*T, V]`，label 也要 reshape 成 `[B*T]`。

---

## 六、总结

### 1. 大模型训练中交叉熵损失函数的常见技巧

| 技巧 | 描述 |
|------|------|
| **ignore_index** | 忽略某些特殊 token（如 padding、[CLS]）的损失计算 |
| **label smoothing** | 对 one-hot 标签进行平滑，缓解过拟合问题 |
| **class weights** | 对类别不平衡的数据加权损失 |
| **sequence-level normalization** | 对每个序列单独归一化损失，避免长序列主导训练 |

---

### 2. 为什么大模型偏爱交叉熵损失？

1. **与最大似然估计一致**  
   最小化交叉熵 ≡ 最大化似然函数，符合统计学习理论。

2. **梯度友好**  
   在配合 softmax 使用时，梯度不会消失或爆炸，适合深层网络训练。

3. **惩罚机制合理**  
   对错误预测有很强的惩罚（因为 $\log(q)$ 在 $q \to 0$ 时趋于无穷大），迫使模型快速修正严重错误。

4. **高效实现**  
   PyTorch、HuggingFace Transformers 等框架对其进行了高度优化，支持大规模并行训练。

---

### 3. 总结

| 场景 | 损失函数定义 |
|------|----------------|
| **多分类任务** | $ L = -\sum_c y_c \log(\hat{y}_c) $ |
| **语言建模** | $ \mathcal{L} = -\sum_t \log P(x_{t+1} \mid x_{1..t}) $ |
| **序列生成** | $ \mathcal{L} = -\sum_t \log P(y_t \mid y_{1..t-1}, x) $ |
