# 资料

- https://github.com/Whu-BrainLab/Embodied-AI-Tutorial  动手做具身智能：从算法到实机部署的“通关”实践指南

# 实践动手

## 大模型

### 手写transformer

### 手写gpt2

## RL

### 强化学习基础

### PPO 四足 isacclab
https://developer.nvidia.com/blog/closing-the-sim-to-real-gap-training-spot-quadruped-locomotion-with-nvidia-isaac-lab/?referrer=grok.com
训练四足机器人 使用3层mlp  在平坦地面的速度跟踪
训练 Spot 机器人学会在平坦地形上跟踪给定的线速度和角速度命令，实现稳定的四足运动

观察空间：线速度、角速度、重力、速度命令、关节位置速度、上一次的动作 噪声
动作空间：12个维度的连续值向量，spot机器人12个自由度 每个腿3个关节
./isaaclab.sh -p source/standalone/workflows/rsl_rl/train.py --task Isaac-Velocity-Flat-Spot-v0 --num_envs 4096 --headless --video --enable_cameras

### PPO 人形 isaaclab

### Teacher-Student（老师-学生）训练架构

## VLA 动作大模型 复现+手写

### act

### difussion policy

### openVLA

### pi0、pi0.5

### 四足机器人带机械臂的抓取放置任务

在isaaclab中对pi0 pi0.5模型进行强化学习，完成抓取放置任务


# 论文阅读准备

## RL

### Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning *****

开源：legged-gym

一句话总结：在isaacgym并行环境中，使用PPO算法训练四足机器人，实现了在平坦地形上不到 4 分钟、在复杂地形导航上不到 20 分钟的训练时间

## VLA 动作大模型

### act

### difussion policy

### openVLA

### pi0、pi0.5

## VLN 导航大模型

### QUAR-VLA **

开源信息：2026.01.04 暂时没有开源数据集和模型，有仿真环境的演示

由西湖大学和浙江大学的研究团队开发，并已被 ECCV 2024 接收。

一句话总结：使用自然语言控制四足机器人进行导航，可以在仿真环境中导航，也可以在真实环境中

数据集：作者自己收集了一个数据集，isaacsim中的仿真数据和真实环境操作机器人的数据，数据就是图像+语言导航指令+机器人的动作

模型架构：openVLA模型架构，基础模型Fuyu-8B，系统频率2hz

改进方向：使用pi架构，提高系统频率

### InternVLA-N1 ****

模型架构：双系统，60hz


# 访谈

## 具身智能：一场需要谦逊与耐心的科学远征

六、关于VLA（视觉-语言-动作）技术路径的思考　　我一直对VLA这条技术路线持保留态度，也通过两个例子与大家分享我的思考。首先可以回想一下：小孩子是先掌握语言能力，还是先具备操作技能？答案显而易见，孩子在语言尚不流利时，就已经展现出强大的动手能力，甚至可以“破坏性地”拆解各种物品。这说明，感知与操作能力的发展早于语言。另一个启发来自我在具身智能大会上听到梅卡曼德创始人邵总分享的一个案例：乌鸦具备执行复杂长序列操作任务的能力，比如使用工具获取食物，但它并没有像人类那样复杂的语言系统。这个现象或许揭示了一个重要事实：操作能力和语言能力之间，并没有必然联系。语言固然是一种高效的人机交互方式，语言模型包含了丰富的知识，也可支撑推理，但它的价值在于“如何用”，而不是“必须用”。当前很多 VLA 模型规模庞大，计算资源消耗高，与其实际能解决的问题相比，性价比并不理想。从长远看，这类模型大概率会被更轻量化、更高效的替代方案所取代。

思考：VA模型可能作为system1更加可靠一些，或者其中更加轻量级的语言模型

作者：周指导BoyuZhou
链接：https://zhuanlan.zhihu.com/p/1906835485990565345
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
