# 资料

- https://github.com/Whu-BrainLab/Embodied-AI-Tutorial  动手做具身智能：从算法到实机部署的“通关”实践指南

# 实践动手

## 大模型

### 手写transformer

- 80%完成 还差增加训练指标比如bleu，ddp训练 单机多卡训练，加速训练

### 手写gpt2

- 80% 完成

### 手写SFT

微调写text2sql的模型

### 手写DPO

### 手写GRPO

grpo刚出来时候，做数学题，下棋的实验，可以手动实验

### 拆解视觉语言模型vlm

## RL

### 强化学习基础

### PPO 四足 isacclab
https://developer.nvidia.com/blog/closing-the-sim-to-real-gap-training-spot-quadruped-locomotion-with-nvidia-isaac-lab/?referrer=grok.com
训练四足机器人 使用3层mlp  在平坦地面的速度跟踪
训练 Spot 机器人学会在平坦地形上跟踪给定的线速度和角速度命令，实现稳定的四足运动

观察空间：线速度、角速度、重力、速度命令、关节位置速度、上一次的动作 噪声
动作空间：12个维度的连续值向量，spot机器人12个自由度 每个腿3个关节
./isaaclab.sh -p source/standalone/workflows/rsl_rl/train.py --task Isaac-Velocity-Flat-Spot-v0 --num_envs 4096 --headless --video --enable_cameras

### PPO 人形 isaaclab


skrl框架支持的算法
choices=["AMP", "PPO", "IPPO", "MAPPO"]

AMP训练人形机器人
https://deepwiki.com/search/amp_280b558c-cd77-4cb4-be00-82f1618995be?mode=fast

### Teacher-Student（老师-学生）训练架构

ETH的论文 Learning Quadrupedal Locomotion over Challenging Terrain (Science Robotics, 2020)

### VLA+RL复现

具身智能VLA论文阅读&复现笔记(3) - vla-r1 - steins的文章 - 知乎
https://zhuanlan.zhihu.com/p/1989798726915602124

## VLA 动作大模型 复现+手写

### act

### difussion policy

### openVLA

### pi0、pi0.5 pi系列

### GROOT1.5 1.6 模型

### 四足机器人带机械臂的抓取放置任务

在isaaclab中对pi0 pi0.5模型进行强化学习，完成抓取放置任务


# 论文阅读准备

## RL

### Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning *****

开源：legged-gym

一句话总结：在isaacgym并行环境中，使用PPO算法训练四足机器人，实现了在平坦地形上不到 4 分钟、在复杂地形导航上不到 20 分钟的训练时间

## VLA 动作大模型

VLA模型的微调+RL微调

强化微调复现论文vla-R1
behavior1K的pi0.5微调，开源方案都是sft微调，如果在开源方案的基础上采用simplevla-RL的方法进行强化微调，会不会继续提升性能呢？

### act

### difussion policy

### openVLA

### pi0、pi0.5

## QUAR-VLA **

开源信息：2026.01.04 暂时没有开源数据集和模型，有仿真环境的演示

由西湖大学和浙江大学的研究团队开发，并已被 ECCV 2024 接收。

一句话总结：使用自然语言控制四足机器人进行导航，可以在仿真环境中导航，也可以在真实环境中

数据集：作者自己收集了一个数据集，isaacsim中的仿真数据和真实环境操作机器人的数据，数据就是图像+语言导航指令+机器人的动作

模型架构：openVLA模型架构，基础模型Fuyu-8B，系统频率2hz

改进方向：使用pi架构，提高系统频率

## InternVLA-N1 ****

模型架构：双系统，60hz


## 【访谈】具身智能：一场需要谦逊与耐心的科学远征

六、关于VLA（视觉-语言-动作）技术路径的思考　　我一直对VLA这条技术路线持保留态度，也通过两个例子与大家分享我的思考。首先可以回想一下：小孩子是先掌握语言能力，还是先具备操作技能？答案显而易见，孩子在语言尚不流利时，就已经展现出强大的动手能力，甚至可以“破坏性地”拆解各种物品。这说明，感知与操作能力的发展早于语言。另一个启发来自我在具身智能大会上听到梅卡曼德创始人邵总分享的一个案例：乌鸦具备执行复杂长序列操作任务的能力，比如使用工具获取食物，但它并没有像人类那样复杂的语言系统。这个现象或许揭示了一个重要事实：操作能力和语言能力之间，并没有必然联系。语言固然是一种高效的人机交互方式，语言模型包含了丰富的知识，也可支撑推理，但它的价值在于“如何用”，而不是“必须用”。当前很多 VLA 模型规模庞大，计算资源消耗高，与其实际能解决的问题相比，性价比并不理想。从长远看，这类模型大概率会被更轻量化、更高效的替代方案所取代。

思考：VA模型可能作为system1更加可靠一些，或者其中更加轻量级的语言模型

作者：周指导BoyuZhou
链接：https://zhuanlan.zhihu.com/p/1906835485990565345
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


## 【论文】MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control

这篇论文是介绍xue bin peng开发的一个模仿学习运动控制框架，集成了ppo、deepminic、amp、add等方法，支持在isaaclab中进行强化学习
实验都需要提供运动的片段 作为参考

xuebinpeng的主页  https://xbpeng.github.io/

https://github.com/xbpeng/MimicKit 提供复现的数据和模型

下一步：详细了解deepminic amp等的思想实现，这些算法大都是基于ppo算法的


## 【论文 2018】DeepMimic

## 【论文 2021.5 ***】AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control

强化学习 运动控制 模仿学习 目标条件强化学习  梯度惩罚

使用ppo训练机器人，但是在训练之前先训练一个鉴别器网络，mlp实现，输入的是机器人的本体姿态和参考动作的姿态进行对比，输出一个分数，表示动作有多像参考动作
这篇论文的核心内容要如何对风格奖励进行建模，然后就可以训练agent来模仿数据集中的走路跑步等姿势
风格奖励的建模就是通过GAN网络实现的，GAN网络给出一个风格奖励，这个奖励驱动策略训练

## 【论文 2022 ***】ASE

## 【论文 2025 ***】ADD

多目标奖励设计问题，自动设计多个目标的奖励权重问题


## 【论文 2024.4】Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer

这篇论文讲的是通过ppo算法在isaacgym中大规模训练人形机器人，然后sim2sim到mujoco，再迁移到真实机器人上sim2real

主要由精心设计的奖励函数，速度奖励，还有步态奖励，基于rsl_rl、legged_gym实现

下一步：在isaaclab中参考这些思想设计ppo算法，训练机器人，isaaclab自带了实现，奖励设计，域随机化

为什么使用mujoco

在 Humanoid-Gym 框架中，引入 MuJoCo 并不是为了**训练**（训练仍然在 Isaac Gym 中进行），而是为了**验证（Validation）**。

1. 物理引擎的“交叉验证” (避免过拟合)
*   **Isaac Gym (PhysX)**：为了实现数千个环境的并行训练，Isaac Gym 底层使用的 PhysX 引擎在某些物理细节上做了一定的简化（以换取速度）。模型有可能会“过拟合”（Overfit）到 PhysX 引擎特有的物理特性或 Bug 上。
*   **MuJoCo**：被公认为机器人领域物理仿真精度最高的“黄金标准”，特别是在接触动力学（Contact Dynamics）方面非常严谨。
*   **作用**：如果一个策略在 Isaac Gym 里跑得很好，但放进 MuJoCo 里立刻摔倒，说明这个策略可能利用了 Isaac Gym 的物理漏洞，在真机上也大概率会失败。反之，如果策略能跨越两个完全不同的物理引擎都能稳定行走，那么它在真实世界成功的概率就会大大增加。

2. 作为“真实世界”的高保真替身
直接把策略从仿真部署到真机（Sim-to-Real）是有风险的，可能会损坏昂贵的机器人硬件。
*   Humanoid-Gym 的作者微调了 MuJoCo 的环境参数，使其尽可能接近真实机器人的动力学特性。
*   **流程**：`Isaac Gym (快速大规模训练)` -> `MuJoCo (高精度模拟测试)` -> `Real World (真机部署)`。
*   MuJoCo 在这里充当了一个**安全过滤器**。只有通过了 MuJoCo 测试的策略，才有资格上真机。

3. 验证 Sim-to-Real 的鲁棒性
论文强调 **Zero-Shot Transfer**（零样本迁移）。
*   在 Isaac Gym 中，我们通常使用“域随机化”（Domain Randomization，比如随机调整摩擦力、质量、推力干扰）来让模型适应各种情况。
*   MuJoCo 提供了一个完全不同的、确定性的物理环境。如果在 Isaac Gym 训练出的策略能够直接在 MuJoCo 中运行（这本身就是一种 Sim-to-Sim），这有力地证明了该策略具有很强的泛化能力（Generalization），足以应对物理参数的变化。

什么是zero-shot，怎么实现

论文使用域随机化，就是对机器人的状态值，和环境参数值，添加噪声，比如机器人的关节角度添加噪声，动力输出缩放，让策略从一个在传感器有噪声、电机时强时弱、通信有延迟、地面时滑时涩的恶劣环境中活下来，那么它在真实世界中大概率也能走得稳

开源项目：Humanoid-gym

## 【论文 2025.4】 ASAP

像humanoid-gym这样的训练，通过域随机化实际上在赌总有一种情况和现实是相同的，也就是现实中的情况和实际机器人应该输出的动作之间有gap
但是这篇论文是直接学习这个gap，比如机器人在模拟环境中输出30的力，但是到了真实环境实际上需要50，那么这个delta模型就会告诉策略，在真实环境中你需要多输出20才可以的


## 【博客】从数学的角度来说，强化学习究竟是在干什么？

从数学的角度来说，强化学习究竟是在干什么？ - Soulflare的回答 - 知乎
https://www.zhihu.com/question/15481741792/answer/1994498790263109516

这篇博客从基本的强化学习原理讲到deepseek r1的训练 很简洁 有指导性


## 【访谈】专访Xue Bin(Jason) Peng：探索人形机器人全身运控的通用控制器 - 王建明的文章 - 知乎
https://zhuanlan.zhihu.com/p/1960271796453512110

访谈说了deepminic amp的发展思想，以及现在sim2rel的gap，现在最大的挑战
重要观点
- 未来我们会看到越来越多生成模型直接作为控制系统的核心架构。
- 大模型未来更多的用在高层规划，低层还是普通的运动控制器
