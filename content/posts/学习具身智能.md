# 资料

- https://github.com/Whu-BrainLab/Embodied-AI-Tutorial  动手做具身智能：从算法到实机部署的“通关”实践指南

# 实践动手

## 大模型

### 手写transformer

### 手写gpt2

## RL

### 强化学习基础

### PPO 四足 isacclab
https://developer.nvidia.com/blog/closing-the-sim-to-real-gap-training-spot-quadruped-locomotion-with-nvidia-isaac-lab/?referrer=grok.com
训练四足机器人 使用3层mlp  在平坦地面的速度跟踪
训练 Spot 机器人学会在平坦地形上跟踪给定的线速度和角速度命令，实现稳定的四足运动

观察空间：线速度、角速度、重力、速度命令、关节位置速度、上一次的动作 噪声
动作空间：12个维度的连续值向量，spot机器人12个自由度 每个腿3个关节
./isaaclab.sh -p source/standalone/workflows/rsl_rl/train.py --task Isaac-Velocity-Flat-Spot-v0 --num_envs 4096 --headless --video --enable_cameras

### PPO 人形 isaaclab

## VLA 动作大模型 复现+手写

### act

### difussion policy

### openVLA

### pi0、pi0.5

### 四足机器人带机械臂的抓取放置任务

在isaaclab中对pi0 pi0.5模型进行强化学习，完成抓取放置任务


# 论文阅读准备

## RL

### Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning *****

开源：legged-gym

一句话总结：在isaacgym并行环境中，使用PPO算法训练四足机器人，实现了在平坦地形上不到 4 分钟、在复杂地形导航上不到 20 分钟的训练时间

## VLA 动作大模型

### act

### difussion policy

### openVLA

### pi0、pi0.5

## VLN 导航大模型

### QUAR-VLA **

开源信息：2026.01.04 暂时没有开源数据集和模型，有仿真环境的演示

由西湖大学和浙江大学的研究团队开发，并已被 ECCV 2024 接收。

一句话总结：使用自然语言控制四足机器人进行导航，可以在仿真环境中导航，也可以在真实环境中

数据集：作者自己收集了一个数据集，isaacsim中的仿真数据和真实环境操作机器人的数据，数据就是图像+语言导航指令+机器人的动作

模型架构：openVLA模型架构，基础模型Fuyu-8B，系统频率2hz

改进方向：使用pi架构，提高系统频率

### InternVLA-N1 ****

模型架构：双系统，60hz
