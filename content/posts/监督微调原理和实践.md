+++ 
date = '2025-06-30' 
draft = false 
title = '监督微调原理和实践' 
categories = ['监督微调'] 
tags = ['SFT', 'Lora'] 
+++

## 一、什么是 Supervised Fine-Tuning (SFT)

### 定义
监督微调（SFT）是指在已有预训练语言模型的基础上，使用带有标签的对话或任务数据对其进行微调。这些数据通常是“问题-答案”、“指令-响应”等形式。

### 数学原理
给定一个预训练的语言模型 $ p\_{\theta}(y | x) $，我们希望优化参数 $ \theta $，使得模型对输入 $ x $ （问题、指令）能够输出目标 $ y $（答案、响应）。

损失函数通常是最小化负对数似然（Negative Log-Likelihood, NLL）：
$$
\mathcal{L}(\theta) = -\sum\_{(x,y)\in D} \log p\_{\theta}(y|x)
$$

其中：
- $ D $ 是监督训练数据集；
- $ x $ 是输入（prompt）；
- $ y $ 是期望输出（response）；

### Loss计算

Transformer 解码器会预测下一个 token，损失只计算 $ y $输出部分的 tokens，忽略 $ x $输入部分。

在 **监督微调**（Supervised Fine-Tuning, SFT）中，整个样本（prompt + completion）都会被输入到模型中，但训练的目标是让模型学会根据 `prompt` 来预测 `completion`。因此：

- **输入模型的完整序列 = prompt + completion**
- **训练目标：让模型学会从 prompt 推理出 completion**


**为什么只计算 completion 部分的损失**

因为我们在做的是一个“生成任务”，希望模型学习的是：**给定 prompt 后，如何生成正确的 response（即 completion）**。

所以，在计算损失时，我们只关心模型对 completion 的预测是否正确，而不希望它去“学习”prompt 内容本身（因为 prompt 是用户输入，不是我们要生成的内容）。


这就像你在教一个学生解题：你给他题目（prompt），他要写出答案（completion）。你不希望他去“记住”题目内容，而是希望他能学会解题思路。


**trl SFTTrainer自动忽略计算prompt损失**

TRL 的 `SFTTrainer` 在内部会自动帮你处理这件事。它通过在数据预处理阶段设置标签（labels），将 prompt 部分的 label 设为 `-100`，这样在计算损失时会被忽略。

例如：

```python
# 假设 input_ids = [token_prompt_1, token_prompt_2, ..., token_comp_1, token_comp_2, ...]
# labels = [-100, -100, ..., token_comp_1, token_comp_2, ...]
```

PyTorch 的交叉熵损失函数（`CrossEntropyLoss`）默认会跳过值为 `-100` 的位置。


## 二、为什么要做SFT


> **为了让预训练语言模型更好地适应特定任务或风格，提升其在实际应用场景中的表现。**


1. **预训练模型 ≠ 完美适配所有任务**  
   预训练模型（如 LLaMA、Qwen、ChatGLM 等）是在大规模通用语料上训练的，虽然具备广泛的语言理解能力，但并不一定擅长处理具体的下游任务（如问答、对话、代码生成等）。

2. **SFT 的作用：让模型“学会听话”**  
   - 让模型理解用户指令（Instruction Tuning）
   - 提升生成内容的质量（更准确、更连贯、更符合人类偏好）
   - 对齐具体应用场景（如客服、教育、医疗等）

3. **是后续 RLHF 流程的基础步骤**  
   SFT 是 RLHF（Reinforcement Learning from Human Feedback）的第一步，后续还会进行 RM（Reward Modeling）和 PPO/DPO 等强化学习阶段。


| 场景 | 是否适合 SFT |
|------|---------------|
| ✅ 你有大量人工标注的高质量 prompt-response 数据 | ✅ 非常适合 |
| ✅ 想让模型更好地理解指令（Instruction Following） | ✅ 推荐使用 SFT |
| ✅ 想让模型输出更像人类风格（比如更自然、礼貌、结构清晰） | ✅ 推荐 SFT |
| ❌ 只想用模型做推理，没有定制化需求 | ❌ 不需要 |
| ❌ 没有标注数据，只有原始语料 | ❌ 不适合，建议继续预训练或尝试 DPO |


**数据量建议**

| 模型大小 | 推荐数据量（示例） |
|----------|-------------------|
| 小型模型（700M~3B） | 1万 ~ 5万条 |
| 中型模型（7B~13B） | 5万 ~ 20万条 |
| 大型模型（30B+） | 20万条以上 |

> ⚠️ 注意：不是越多越好，而是越**高质量**越好！



## 三、使用trl库对Qwen2.5进行SFT

### 数据集准备


1. 对话格式 (Conversational Format)
```json
{"messages": [{"role": "system", "content": "You are helpful"}, {"role": "user", "content": "What's the capital of France?"}, {"role": "assistant", "content": "..."}]}
```

1. 指令格式 (Instruction Format)  
```json
{"prompt": "<prompt text>", "completion": "<ideal generated text>"}
```

### trl-lib/Capybara数据集

[trl-lib/Capybara](https://huggingface.co/datasets/trl-lib/Capybara)是hugging face维护的一个对话格式数据集，包含多个数据来源的对话数据 


### Qwen2.5 SFT Lora代码

```python

from datasets import load_dataset  
from transformers import AutoModelForCausalLM, AutoTokenizer  
from trl import SFTConfig, SFTTrainer  
from peft import LoraConfig  
  
# 1. 加载模型和分词器  
model_name = "Qwen/Qwen2.5-0.5B"  
model = AutoModelForCausalLM.from_pretrained(  
    model_name,  
    torch_dtype="bfloat16",  
    device_map="auto"  
)  
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  
  
# 2. 加载数据集  
dataset = load_dataset("trl-lib/Capybara", split="train")  
  
# 3. 配置 PEFT (LoRA)  
peft_config = LoraConfig(  
    r=16,  
    lora_alpha=32,  
    lora_dropout=0.05,  
    target_modules="all-linear",  
    modules_to_save=["lm_head", "embed_tokens"],  
    task_type="CAUSAL_LM",  
)  
  
# 4. 配置训练参数  
training_args = SFTConfig(  
    output_dir="Qwen2.5-0.5B-SFT",  
    num_train_epochs=1,  
    per_device_train_batch_size=2,  
    gradient_accumulation_steps=8,  
    gradient_checkpointing=True,  
    learning_rate=2.0e-4,  
    max_length=512,  
    packing=True,  
    logging_steps=25,  
    eval_strategy="steps",  
    eval_steps=100,  
    bf16=True,  
    report_to="none",  
    push_to_hub=False  
)  
  
# 5. 创建训练器  
trainer = SFTTrainer(  
    model=model,  
    args=training_args,  
    train_dataset=dataset,  
    processing_class=tokenizer,  
    peft_config=peft_config  
)  
  
# 6. 开始训练  
trainer.train()  
  
# 7. 保存模型  
trainer.save_model()

```

### Qwen2.5 全参SFT代码

```python

from datasets import load_dataset  
from transformers import AutoModelForCausalLM, AutoTokenizer  
from trl import SFTConfig, SFTTrainer  
  
# 1. 加载模型和分词器  
model_name = "Qwen/Qwen2.5-0.5B"  
model = AutoModelForCausalLM.from_pretrained(  
    model_name,  
    torch_dtype="bfloat16",  
    device_map="auto"  
)  
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)  
  
# 2. 加载数据集  
dataset = load_dataset("trl-lib/Capybara", split="train")  
  
# 3. 配置训练参数（无 PEFT）  
training_args = SFTConfig(  
    output_dir="Qwen2.5-0.5B-SFT",  
    num_train_epochs=1,  
    per_device_train_batch_size=1,  # 全参数训练需要更小的批次大小  
    gradient_accumulation_steps=16,  # 增加梯度累积步数  
    gradient_checkpointing=True,  
    learning_rate=5.0e-6,  # 全参数训练使用更小的学习率  
    max_length=512,  
    packing=True,  
    logging_steps=25,  
    eval_strategy="steps",  
    eval_steps=100,  
    bf16=True,  
    report_to="none",  
    push_to_hub=False,  
    warmup_steps=100,  
    save_steps=500,  
    save_total_limit=2  
)  
  
# 4. 创建训练器（不传入 peft_config）  
trainer = SFTTrainer(  
    model=model,  
    args=training_args,  
    train_dataset=dataset,  
    processing_class=tokenizer  
)  
  
# 5. 开始训练  
trainer.train()  
  
# 6. 保存模型  
trainer.save_model()

```
