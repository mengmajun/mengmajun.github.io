+++
date = '2025-05-29'
draft = false
title = '大模型经典论文集合'
categories = ['人工智能', '大模型']
tags = ['Transformer', 'Attention机制', '经典论文']
+++

### **一、架构基石**
1. **《Attention is All You Need》**（2017）  
   - **作者**：Vaswani et al. (Google)  
   - **链接**：[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)  
   - **核心贡献**：提出 **Transformer 架构**，用**自注意力机制**替代RNN/CNN，解决长距离依赖问题，支持并行训练。  
   - **意义**：奠定所有大模型的技术基础，NLP领域的革命性起点。  

---

### **二、预训练范式确立**
2. **《BERT: Pre-training of Deep Bidirectional Transformers》**（2018）  
   - **作者**：Devlin et al. (Google)  
   - **链接**：[arXiv:1810.04805](https://arxiv.org/abs/1810.04805)  
   - **核心贡献**：提出 **掩码语言建模（MLM）** 预训练目标，实现双向上下文理解。  
   - **意义**：确立“预训练+微调”范式，推动NLP任务性能飞跃。  
   
3. **《Improving Language Understanding by Generative Pre-Training》（GPT-1）**（2018）  
   - **作者**：Radford et al. (OpenAI)  
   - **链接**：[OpenAI Blog](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
   - **核心贡献**：首次用 **自回归语言建模** 预训练Transformer解码器。  
   - **意义**：奠定生成式大模型技术路线（GPT系列雏形）。  

---

### **三、规模效应与能力涌现**
4. **《Language Models are Unsupervised Multitask Learners》（GPT-2）**（2019）  
   - **作者**：Radford et al. (OpenAI)  
   - **链接**：[arXiv:1904.00962](https://arxiv.org/abs/1904.00962)  
   - **核心贡献**：证明 **模型规模化（1.5B参数）** 可触发**零样本学习**能力。  
   - **意义**：首次揭示“规模效应”，引发大模型参数竞赛。  
   
5. **《Language Models are Few-Shot Learners》（GPT-3）**（2020）  
   - **作者**：Brown et al. (OpenAI)  
   - **链接**：[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)  
   - **核心贡献**：构建 **1750亿参数模型**，实现**上下文学习（In-Context Learning）**。  
   - **意义**：证明“大力出奇迹”，推动Prompt工程成为核心交互方式。  

---

### **四、高效训练与对齐**
6. **《LoRA: Low-Rank Adaptation of Large Language Models》**（2021）  
   - **作者**：Hu et al. (Microsoft)  
   - **链接**：[arXiv:2106.09685](https://arxiv.org/abs/2106.09685)  
   - **核心贡献**：提出 **低秩适配** 微调法，仅训练0.1%参数逼近全量微调效果。  
   - **意义**：降低大模型定制门槛，成为开源社区微调标配。  
   
7. **《Training language models to follow instructions with human feedback》（InstructGPT）**（2022）  
   - **作者**：Ouyang et al. (OpenAI)  
   - **链接**：[arXiv:2203.02155](https://arxiv.org/abs/2203.02155)  
   - **核心贡献**：设计 **RLHF三阶段框架**（监督微调+奖励模型+强化学习）。  
   - **意义**：解决模型对齐问题，ChatGPT的核心技术基础。  

---

### **五、多模态突破**
8. **《Learning Transferable Visual Models From Natural Language Supervision》（CLIP）**（2021）  
   - **作者**：Radford et al. (OpenAI)  
   - **链接**：[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)  
   - **核心贡献**：通过**图文对比学习**对齐跨模态语义空间。  
   - **意义**：打通文本-图像理解，催生多模态大模型浪潮。  
   
9. **《High-Resolution Image Synthesis with Latent Diffusion Models》（Stable Diffusion）**（2022）  
   - **作者**：Rombach et al. (LMU Munich)  
   - **链接**：[arXiv:2112.10752](https://arxiv.org/abs/2112.10752)  
   - **核心贡献**：在**潜空间训练扩散模型**，大幅降低图像生成计算成本。  
   - **意义**：推动AIGC普及，开源生态爆发起点。  

---

### **六、开源生态**
10. **《LLaMA: Open and Efficient Foundation Language Models》**（2023）  
    - **作者**：Touvron et al. (Meta)  
    - **链接**：[arXiv:2302.13971](https://arxiv.org/abs/2302.13971)  
    - **核心贡献**：发布**开源可商用**的7B-65B参数模型。  
    - **意义**：打破闭源垄断，催生Alpaca/Vicuna等数百个衍生模型。  

---

### **关键演进脉络总结**
| **阶段**       | **核心突破**                | **代表论文**   | **解决的关键问题**         |
|----------------|----------------------------|---------------|--------------------------|
| 架构革新       | Transformer自注意力        | Attention is All You Need | 序列建模效率瓶颈     |
| 预训练范式     | 掩码建模 vs 自回归建模     | BERT, GPT-1   | 如何利用无标注数据预训练 |
| 规模效应       | 参数扩大→能力涌现          | GPT-2, GPT-3  | 小模型无法实现通用智能   |
| 效率与对齐     | 轻量化微调+人类偏好对齐    | LoRA, InstructGPT | 计算成本高、模型行为不可控 |
| 多模态理解     | 图文联合表征               | CLIP          | 单一模态局限性           |
| 生成式AI普及   | 高效图像生成               | Stable Diffusion | 创作工具民主化         |
| 开源生态       | 可商用基础模型             | LLaMA         | 闭源模型垄断问题         |

---
