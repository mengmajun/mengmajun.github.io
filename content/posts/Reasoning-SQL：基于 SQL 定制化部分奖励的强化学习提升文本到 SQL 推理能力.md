+++ 
date = '2025-07-09' 
draft = false 
title = 'Reasoning-SQL：基于 SQL 定制化部分奖励的强化学习提升文本到 SQL 推理能力' 
categories = ['论文阅读', 'Text2SQL'] 
tags = ['Text2SQL', '强化学习', 'GRPO'] 
+++


[Reasoning-SQL：基于 SQL 定制化部分奖励的强化学习提升文本到 SQL 推理能力](https://www.arxiv.org/abs/2503.23157)

- 论文时间2025-03-29
- 论文提供了伪代码
- 论文基于Qwen2.5-coder模型


## 摘要

文本到SQL（Text-to-SQL）是一项具有挑战性的任务，涉及多个推理密集型子任务，包括自然语言理解、数据库模式 comprehension（理解）和精确的SQL查询构建。现有方法通常依赖带有归纳偏差的手工设计推理路径，这可能会限制其整体效果。受近期推理增强模型（如DeepSeek R1（Guo等人，2025）和OpenAI o1（Jaech等人，2024））成功的启发——这些模型通过奖励驱动的自我探索有效提升了推理能力和泛化性，我们提出了一套专为文本到SQL任务定制的新型部分奖励。该奖励集包括模式链接、AI反馈、n-gram相似度和语法检查，专门设计用于解决强化学习（RL）中普遍存在的奖励稀疏问题。借助组相对策略优化（GRPO）算法，我们的方法能明确促使大型语言模型（LLMs）发展生成准确SQL查询所需的内在推理能力。通过不同规模的模型实验，我们证明：与监督微调（SFT）相比，采用我们提出的奖励进行纯强化学习训练，能持续实现更高的准确率和更优的泛化性。值得注意的是，在BIRD基准测试中，我们经强化学习训练的14B参数模型显著优于更大的专有模型，例如比o3-mini高出4%，比Gemini-1.5-Pro-002高出3%。这些结果凸显了我们提出的带部分奖励的强化学习训练框架在提升文本到SQL任务准确率和推理能力方面的有效性。


## 引言

基于强化学习（RL）训练的核心组成部分是奖励函数，它对指导模型至关重要。**在文本到SQL（Text-to-SQL）任务中，最直观的奖励当属执行准确性。然而，正如Nguyen等人（2025）所指出的，其二元性和稀疏性意味着，当模型仅部分捕捉到正确的逻辑或模式关系时，这种奖励能提供的反馈十分有限**。奖励稀疏性是许多基于强化学习的训练方法中一个显著的挑战，常常阻碍策略模型的优化（Blier & Ollivier, 2021；Rengarajan等人, 2022；Jaderberg等人, 2016）。为解决这一问题，我们设计了一种复合奖励，它整合了多种部分奖励（即LLM作为裁判奖励、语法检查奖励、模式链接奖励和n-gram相似度奖励），以应对文本到SQL任务中的奖励稀疏问题。我们采用组相对策略优化（GRPO）算法（Shao等人, 2024）来有效整合这些奖励信号。通过为每个输入生成多个候选查询并进行相对评估，我们的方法提供了一种稳健且信息丰富的反馈机制，能直接优化中间推理过程和最终执行准确性。


## 奖励设计

执行准确性奖励（RLEF）。该奖励直接评估生成的SQL查询在数据库中执行时是否能产生正确结果。对于每个候选查询，我们将其与真实查询一同执行，并对比两者的输出结果，完全匹配的查询将获得满分奖励。尽管执行准确性是与下游目标一致的终极基准，但其二元性导致奖励信号较为稀疏。这种稀疏性是一个局限，因为许多查询可能包含部分正确的逻辑，但仍无法产生完全正确的输出，因此得不到任何奖励。


大语言模型作为法官奖励（RLAIF，Reinforcement Learning from AI Feedback）。为了补充二元的执行反馈，我们采用大语言模型作为评判工具。通过一个专门设计的提示词（详见附录A.3部分）——该提示词包含用于比较SQL查询的具体标准，大语言模型会将候选查询与真实查询进行对比评估。**这种评估仅针对执行准确性为零的查询，依据逻辑一致性、结构相似性和语义正确性等标准对其进行判断**。这一机制能提供细致的部分奖励，区分不同错误答案的差异程度。图1中的“x”代表正确查询，因此无需额外的AI反馈。

语法检查奖励。语法有效性是任何查询具备意义或可执行性的基本前提。我们设计了相应的奖励机制：当生成的SQL查询语法有效且执行无错误时，无论其是否返回完全正确的结果，都会被赋予一个正分。这种奖励策略通过奖励语法正确性来帮助区分不同的错误查询，从而引导模型优先生成格式规范的查询。

模式链接奖励。在文本到SQL（Text-to-SQL）任务中，准确的模式链接至关重要（Wang等人，2019；Pourreza & Rafiei，2024a；Talaei等人，2024；Caferoğlu & Ulusoy，2024），因为模型必须正确识别并引用相关的表和列。我们的模式链接奖励通过计算候选查询中使用的模式项集合与黄金查询中模式项集合的Jaccard similarity来量化，直接解决了将自然语言实体映射到数据库模式正确部分这一难题。图2展示了针对一个预测查询和黄金查询对计算模式链接奖励的示例。（模式链接奖励就是说黄金查询和模型生成的查询使用的字段是否一致）

n-gram相似度奖励。SQL的结构化特性使其能够利用令牌级重叠作为相似性度量。n-gram相似度奖励计算候选SQL查询与黄金SQL查询中n-gram的杰卡德相似度。鉴于SQL固有的层级语法和结构化格式，即使存在细微差异，这种奖励也能促进与正确查询在词汇和句法结构上的对齐。图2展示了本文所采用的、使用二元语法（N=2）计算n-gram相似度的过程。（就是说两个查询的文本相似度）


格式奖励。最后，我们加入了格式奖励，用于鼓励模型遵循预定义的输出结构（例如，使用<reasoning>和<|FunctionCallBegin|>标签）。符合这种模式的输出会获得奖励加成，从而提高输出的清晰度和一致性。这种结构还能触发策略模型的零样本思维链推理，随着训练的推进，模型为了优化奖励，其推理能力会逐步提升。


## 实验设置

### 基线和评估标准

基准测试与评估指标。我们在BIRD训练集（Li等人，2024c）上训练模型，该训练集包含来自70个数据库的9428个“问题-SQL”对，覆盖航空、电影、销售等多个领域。为解决已知的查询含噪和歧义问题（Pourreza等人，2024；Talaei等人，2024；Li等人，2024b），我们过滤掉了被Gemini-2.0-flash和GPT-4o均标记为错误的样本，最终得到8026个训练样本。

评估方面，我们主要使用BIRD基准测试，并纳入Spider、Spider-DK（Gan等人，2021b）和Spider-Syn（Gan等人，2021a）以评估模型的泛化能力。其中，BIRD包含多样化的模式和含噪查询，能反映现实世界中的SQL挑战；Spider则提供了分布外测试，其查询来自广泛的数据库；Spider-Syn通过使用与模式相关的同义词对问题进行转述，以测试模型的鲁棒性；Spider-DK通过在Spider查询中添加领域知识进行修改，以模拟真实场景中的转述情况。我们报告的评估指标为执行准确率（EX），该指标要求查询结果与正确结果的行完全匹配。


## 结果
如表1所示，与仅使用稀疏的执行准确性奖励进行训练相比，采用我们提出的部分奖励函数能带来更高的性能。值得注意的是，每种奖励的加入都会直接提升其对应的指标。例如，添加语法奖励使语法检查得分提高了1.37，引入模式奖励使模式项杰卡德相似度提升了0.78，纳入n-gram奖励则让n-gram杰卡德相似度增加了3.92。这些用向上箭头和差值突出显示的改进表明，每种奖励在针对性提升对应指标的同时，也对整体执行准确性有所贡献。

总体而言，我们提出的方法使基础模型的性能提升了6.77%，而仅通过传统监督微调（SFT）训练的模型性能提升为4.11%。此外，将经过GRPO训练且包含推理过程的模型与经过GRPO训练但仅预测SQL查询的模型进行对比，我们发现前者性能提升了2%，这体现了逐步推理对SQL生成的重要性。我们还研究了模型在不同测试时计算预算下的表现，更多细节详见附录A.4。


## 什么是奖励稀疏

“奖励稀疏”（Sparse Rewards）是强化学习（Reinforcement Learning, RL）中的一个核心概念，指的是智能体在与环境交互的过程中，只有极少数特定时刻能获得明确的奖励信号，而绝大多数情况下得不到任何奖励反馈的现象。


### 具体理解
- **奖励信号的“稀缺性”**：在稀疏奖励场景中，智能体可能需要执行大量动作（甚至完成一长串复杂行为链），才能在某个关键节点（如达成最终目标）获得一次奖励。例如：
  - 游戏中，只有通关时才获得奖励，中间的所有操作（移动、打怪等）都无奖励；
  - 机器人导航任务中，只有到达终点时才有奖励，中途的探索行为均无反馈。
- **与“密集奖励”的对比**：密集奖励场景中，智能体的每一步有效动作（如靠近目标、规避障碍）都会获得即时奖励，而稀疏奖励缺乏这种“中间引导”，导致智能体难以判断自身行为的价值。


### 稀疏奖励带来的挑战
1. **探索效率低下**：由于缺乏中间奖励，智能体无法通过反馈判断哪些动作更接近目标，容易陷入随机探索的“试错陷阱”，甚至长期无法找到有效策略。
2. **信用分配难题**：当最终获得奖励时，智能体难以确定在之前的一系列动作中，哪些步骤是关键贡献者（即“如何将奖励分配给正确的行为”）。例如，机器人成功开门的奖励，无法直接对应到“旋转门把手”“推开门”等具体动作的价值。
3. **容易陷入局部最优**：若智能体偶然在某个非目标状态获得奖励（或误认为某动作有效），可能会反复执行无效行为，难以探索到更优策略。
